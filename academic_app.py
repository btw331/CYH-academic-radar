import streamlit as st
import google.generativeai as genai
import requests
import pandas as pd
import re
import json
from urllib.parse import unquote
import time

# ==========================================
# 0. åŸºç¤è¨­å®šèˆ‡ CSS
# ==========================================
st.set_page_config(page_title="å­¸è¡“é›·é” V12.9 (Future Proof)", page_icon="ğŸ§¬", layout="wide")

st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+TC:wght@400;500;700&family=Roboto+Mono&display=swap');
    html, body, [class*="css"] { font-family: 'Noto Sans TC', sans-serif; color: #333; }
    
    .report-container {
        background-color: #fdfbf7; 
        border: 2px solid #5d4037; 
        border-radius: 12px;
        padding: 30px; 
        margin-bottom: 25px; 
        box-shadow: 0 4px 15px rgba(0,0,0,0.08);
        line-height: 1.7;
    }
    
    .pi-box {
        background-color: #e3f2fd; border: 2px solid #1565c0; border-radius: 12px;
        padding: 25px; margin-top: 20px; margin-bottom: 25px;
    }
    
    .source-badge {
        display: inline-block; padding: 4px 12px; border-radius: 20px;
        font-size: 0.85em; font-weight: 700; margin-bottom: 15px;
        background-color: #e8f5e9; color: #2e7d32; border: 1px solid #a5d6a7;
    }
    
    .bib-container { background-color: #fff8e1; padding: 20px; border-radius: 10px; border: 1px solid #ffe082; margin-top: 20px; }
    
    .auth-tag-first { color: #d32f2f; font-weight: bold; }
    .auth-tag-last { color: #1976d2; font-weight: bold; }
    
    .search-card {
        background-color: #ffffff; padding: 20px; border-radius: 10px;
        border: 1px solid #e0e0e0; margin-bottom: 15px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05); transition: transform 0.2s;
    }
    .search-card:hover { transform: translateY(-3px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); }
    .sc-title { font-size: 1.1em; font-weight: bold; color: #1a237e; margin-bottom: 8px; }

    .chat-box { background-color: #f1f8e9; padding: 15px; border-radius: 10px; border: 1px solid #c5e1a5; margin-top: 10px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. æ ¸å¿ƒæœå°‹å¼•æ“
# ==========================================
HEADERS = {"User-Agent": "AcademicRadar/12.9"}

LIGHT_FIELDS = "paperId,title,year,citationCount,venue,authors.name,references.paperId,references.citationCount,references.year,citations.paperId,citations.citationCount,citations.year"
RICH_FIELDS = "paperId,title,year,citationCount,venue,authors.name,authors.authorId,abstract,tldr"
BROAD_FIELDS = "paperId,title,year,citationCount,venue,authors.name,abstract,tldr"
AUTHOR_FIELDS = "authorId,name,citationCount,hIndex,paperCount,papers.title,papers.year,papers.citationCount,papers.venue"

@st.cache_data(ttl=3600, show_spinner=False)
def search_broad_papers(query, limit=10):
    if not query: return []
    try:
        r = requests.get("https://api.semanticscholar.org/graph/v1/paper/search", params={"query": query, "limit": limit, "fields": BROAD_FIELDS}, headers=HEADERS, timeout=10)
        if r.status_code == 200: return r.json().get('data', [])
    except: pass
    return []

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_network_skeleton(user_input):
    clean_input = unquote(user_input).strip().replace('"', '')
    lookup_id = None
    doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', clean_input)
    arxiv_match = re.search(r'(\d{4}\.\d{4,5})', clean_input)
    
    if doi_match: lookup_id = f"DOI:{doi_match.group(1)}"
    elif arxiv_match: lookup_id = f"arXiv:{arxiv_match.group(1)}"
    
    def fetch(pid):
        try:
            r = requests.get(f"https://api.semanticscholar.org/graph/v1/paper/{pid}", params={"fields": LIGHT_FIELDS}, headers=HEADERS, timeout=10)
            if r.status_code == 200: return r.json()
        except: pass
        return None

    hero = fetch(lookup_id) if lookup_id else None
    if not hero:
        try:
            r = requests.get("https://api.semanticscholar.org/graph/v1/paper/search", params={"query": clean_input, "limit": 1, "fields": "paperId"}, headers=HEADERS)
            if r.status_code == 200 and r.json().get('data'):
                hero = fetch(r.json()['data'][0]['paperId'])
        except: pass
        
    if not hero or not hero.get('paperId'): return None
    
    refs = sorted([r for r in (hero.get('references') or []) if r.get('paperId')], key=lambda x: (x.get('citationCount') or 0), reverse=True)
    cites = sorted([c for c in (hero.get('citations') or []) if c.get('paperId')], key=lambda x: (x.get('year') or 0), reverse=True)
    
    return {'hero': hero, 'all_ancestors': refs, 'all_descendants': cites}

def enrich_segment(paper_objects):
    if not paper_objects: return []
    ids = [p['paperId'] for p in paper_objects if p.get('paperId')]
    if not ids: return paper_objects
    
    enriched_map = {}
    try:
        r = requests.post("https://api.semanticscholar.org/graph/v1/paper/batch", params={"fields": RICH_FIELDS}, json={"ids": ids}, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            for p in r.json():
                if p: enriched_map[p['paperId']] = p
    except: pass
        
    enriched_list = []
    for p in paper_objects:
        pid = p['paperId']
        if pid in enriched_map:
            full_data = enriched_map[pid]
            if 'code' in p: full_data['code'] = p['code']
            enriched_list.append(full_data)
        else:
            enriched_list.append(p)
    return enriched_list

def fetch_author_profile_no_cache(author_id):
    try:
        r = requests.get(f"https://api.semanticscholar.org/graph/v1/author/{author_id}", params={"fields": AUTHOR_FIELDS}, headers=HEADERS, timeout=10)
        if r.status_code == 200: return r.json()
    except: pass
    return None

# ==========================================
# 2. AI Prompt
# ==========================================
def generate_deep_analysis_classic(hero, ancestors, descendants, api_key, model_name):
    genai.configure(api_key=api_key)
    
    def format_paper(p, code):
        title = p.get('title', 'Unknown Title')
        year = p.get('year', 'N/A')
        cite = p.get('citationCount', 0)
        
        auth_list = p.get('authors', [])
        if not auth_list: auth_str = "Unknown"
        elif len(auth_list) <= 4:
            auth_str = ", ".join([a.get('name','?') for a in auth_list])
        else:
            first = auth_list[0].get('name', '?')
            last_3 = [a.get('name','?') for a in auth_list[-3:]]
            auth_str = f"First:{first} ... Last3:{', '.join(last_3)}"
        
        return f"[{code}] {title} ({year}) | {auth_str} | Cited:{cite}"

    context = f"ä¸»è§’è«–æ–‡: {format_paper(hero, 'Hero')}\n\n"
    context += "ã€ç¥–å…ˆæ–‡ç»ã€‘:\n" + "\n".join([format_paper(a, a.get('code','A')) for a in ancestors]) + "\n\n"
    context += "ã€å¾Œä»£æ–‡ç»ã€‘:\n" + "\n".join([format_paper(d, d.get('code','D')) for d in descendants])

    system_prompt = """
    ä½ æ˜¯ä¸€ä½ç²¾é€šã€Œå­¸è¡“ç³»è­œå­¸ã€çš„ AI å°ˆå®¶ã€‚
    è«‹åŸºæ–¼æä¾›çš„è«–æ–‡åˆ—è¡¨ï¼Œé€²è¡Œæ·±åº¦çš„æ•¸æ“šæ¨è«–ã€æ¦‚å¿µæµè®Šåˆ†æï¼Œä¸¦é æ¸¬æœªä¾†çš„å¯èƒ½æ€§ã€‚
    
    ã€é‡è¦æŒ‡ä»¤ã€‘ï¼š
    1. **èªè¨€**ï¼šæ‰€æœ‰è¼¸å‡ºå¿…é ˆä½¿ç”¨ **ç¹é«”ä¸­æ–‡ (Traditional Chinese, Taiwan)**ã€‚
    2. **è¡¨æ ¼å‘ˆç¾**ï¼šæ¦‚å¿µæµè®Šè«‹å‹™å¿…ä½¿ç”¨ **Markdown è¡¨æ ¼** å‘ˆç¾ã€‚
    
    ã€è¼¸å‡ºå ±å‘Šæ ¼å¼ã€‘ï¼š
    ### ğŸ“œ å­¸è¡“é›·é”æ·±åº¦å ±å‘Š
    
    #### 1. ğŸŒŠ æ¦‚å¿µæµè®Šè¡¨ (Concept Flow Table)
    | éšæ®µ | æ ¸å¿ƒé—œéµè© | æ¼”è®Šæè¿° |
    | :--- | :--- | :--- |
    | **Aç³»åˆ— (èµ·æº)** | ... | ... |
    | **Hero (è½‰æŠ˜)** | ... | ... |
    | **Dç³»åˆ— (æ‡‰ç”¨)** | ... | ... |
    
    #### 2. ğŸ§© é ˜åŸŸåˆ†é¡èˆ‡èšé¡
    * **ç¾¤çµ„ A (ç†è«–åŸºçŸ³)**ï¼š[A1], [A3]...
    * **ç¾¤çµ„ B (æ–¹æ³•çªç ´)**ï¼š[Hero], [D1]...
    
    #### 3. ğŸ‘‘ é ˜åŸŸé ˜è¢–èˆ‡å¸«æ‰¿
    * **æ ¸å¿ƒå¯¦é©—å®¤ (PI)**ï¼š(è§€å¯Ÿä½œè€…ç¾¤çš„æœ€å¾Œå¹¾ä½ï¼Œæ¨è«–æ ¸å¿ƒå¯¦é©—å®¤)
    * **ç¬¬ä¸€ä½œè€… (åŸ·è¡Œè€…)**ï¼š(è§€å¯Ÿç¬¬ä¸€ä½œè€…çš„è²¢ç»)
    
    #### 4. ğŸ”— æŠ€è¡“æ¼”é€²è©³è§£
    **4.1 âª å‘å‰æº¯æº**
    * **[A?]** (PI: ...): **[è²¢ç»]** ... 
    
    **4.2 â© å‘å¾Œå±•æœ›**
    * **[D?]** (PI: ...): **[è²¢ç»]** ... 
    
    #### 5. ğŸ”® æœªä¾†å¯èƒ½æ€§åœ“éŒ (The Cone of Possibilities)
    *(é‡å° Hero è«–æ–‡ï¼Œé æ¸¬æœªä¾†)*
    * **ğŸ¯ æ ¸å¿ƒ (Probable)**ï¼š...
    * **ğŸš€ æ“´å±• (Plausible)**ï¼š...
    * **ğŸŒŒ é‚Šç•Œ (Possible)**ï¼š...
    """
    try:
        model = genai.GenerativeModel(model_name)
        return model.generate_content(system_prompt + context).text
    except Exception as e: return f"åˆ†æå¤±æ•—: {str(e)}"

def generate_author_analysis(author_name, selected_papers, api_key, model_name):
    genai.configure(api_key=api_key)
    
    papers_str = "\n".join([f"- {p.get('title', 'Unknown')} ({p.get('year', 'N/A')}) | Cited: {p.get('citationCount', 0)}" for p in selected_papers])
    
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ã€Œå­¸è¡“æ˜Ÿæ¢ã€ã€‚è«‹åˆ†æé€™ä½ PI (æˆ–ç ”ç©¶å“¡)ã€‚
    ã€æ³¨æ„ã€‘ï¼š**å·²æ’é™¤åŒååŒå§“çš„å¹²æ“¾è³‡æ–™**ï¼Œä»¥ä¸‹æä¾›çš„è«–æ–‡ç¢ºå®šçš†ç‚ºåŒä¸€äººæ‰€è‘—ã€‚
    
    ã€æª”æ¡ˆã€‘å§“å: {author_name}
    ã€ç¶“ç¢ºèªçš„ä»£è¡¨ä½œã€‘:
    {papers_str}
    
    ã€ä»»å‹™ã€‘ï¼šè«‹ç”¨**æ¢åˆ—å¼**åˆ†æï¼š
    1. **å­¸è¡“æ±Ÿæ¹–åœ°ä½** (æ˜¯è³‡æ·±å¤§ä½¬ã€å¯¦é©—å®¤ä¸»æŒäººï¼Œé‚„æ˜¯æ–°éŠ³ç ”ç©¶å“¡ï¼Ÿ)
    2. **æ ¸å¿ƒç ”ç©¶ç‰ˆåœ–** (æ ¹æ“šä¸Šè¿°è«–æ–‡ï¼Œç²¾æº–å®šä½å…¶å°ˆé•·)
    3. **ç ”ç©¶é¢¨æ ¼èˆ‡å°ˆé•·**
    """
    try:
        model = genai.GenerativeModel(model_name)
        return model.generate_content(system_prompt).text
    except Exception as e: return f"åˆ†æå¤±æ•—: {str(e)}"

def ask_historian(question, context_data, api_key, model_name):
    genai.configure(api_key=api_key)
    prompt = f"""ä½ æ˜¯ä¸€ä½å­¸è¡“é¡§å•ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\nèƒŒæ™¯ï¼š{str(context_data)[:3000]}\nå•é¡Œï¼šã€Œ{question}ã€"""
    try:
        model = genai.GenerativeModel(model_name)
        return model.generate_content(prompt).text
    except: return "å›ç­”å¤±æ•—"

def generate_multilingual_abstract(text_content, api_key, model_name):
    genai.configure(api_key=api_key)
    prompt = f"""è«‹å°‡å ±å‘Šç¸½çµç‚º **100 å­—æ‘˜è¦**ã€‚è¼¸å‡ºï¼šç¹é«”ä¸­æ–‡ã€Englishã€æ—¥æœ¬èªã€‚\nå…§å®¹ï¼š\n{text_content[:2000]}"""
    try:
        model = genai.GenerativeModel(model_name)
        return model.generate_content(prompt).text
    except: return "æ‘˜è¦ç”Ÿæˆå¤±æ•—"

# å­˜æª”åŠŸèƒ½
def export_state_to_json():
    data = {k: st.session_state[k] for k in ['skeleton', 'full_lineage', 'offsets', 'deep_dive_result', 'pi_analysis_result'] if k in st.session_state}
    return json.dumps(data, default=str)

# ==========================================
# 3. UI é‚è¼¯
# ==========================================
if 'skeleton' not in st.session_state: st.session_state.skeleton = None
if 'full_lineage' not in st.session_state: st.session_state.full_lineage = {'hero': {}, 'ancestors': [], 'descendants': []}
if 'offsets' not in st.session_state: st.session_state.offsets = {'a': 0, 'd': 0}
if 'deep_dive_result' not in st.session_state: st.session_state.deep_dive_result = None
if 'pi_analysis_result' not in st.session_state: st.session_state.pi_analysis_result = None
if 'chat_history' not in st.session_state: st.session_state.chat_history = []
if 'pre_fill_doi' not in st.session_state: st.session_state.pre_fill_doi = ""
if 'read_only_mode' not in st.session_state: st.session_state.read_only_mode = False
if 'pi_raw_data' not in st.session_state: st.session_state.pi_raw_data = None 

with st.sidebar:
    st.title("ğŸ”¬ åƒæ•¸è¨­å®š")
    api_key = st.text_input("Gemini API Key", type="password")
    model_name = st.selectbox("æ¨¡å‹", ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.5-flash-lite"], index=0)
    
    st.divider()
    st.markdown("### ğŸ“¥ çŸ¥è­˜åº«å­˜æª”")
    if st.session_state.deep_dive_result:
        st.download_button("ä¸‹è¼‰é€²åº¦ (JSON)", export_state_to_json(), "radar_fix.json", "application/json", help="å®Œæ•´è³‡æ–™å‚™ä»½")
        st.download_button("ä¸‹è¼‰å ±å‘Š (.md)", st.session_state.deep_dive_result, "academic_report.md", "text/markdown")
    
    with st.expander("ğŸ“‚ è®€å–èˆŠæª”æ¡ˆ (JSON/MD)", expanded=True):
        uploaded_file = st.file_uploader("æ‹–æ›³æª”æ¡ˆåˆ°æ­¤", type=["json", "md"])
        if uploaded_file:
            try:
                if uploaded_file.name.endswith(".json"):
                    data = json.load(uploaded_file)
                    for k, v in data.items(): st.session_state[k] = v
                    st.session_state.read_only_mode = False
                    st.toast("âœ… JSON é€²åº¦é‚„åŸæˆåŠŸï¼")
                    time.sleep(1)
                    st.rerun()
                elif uploaded_file.name.endswith(".md"):
                    content = uploaded_file.read().decode("utf-8")
                    st.session_state.deep_dive_result = content
                    st.session_state.read_only_mode = True
                    st.toast("ğŸ“– é€²å…¥ç´”é–±è®€æ¨¡å¼")
                    time.sleep(1)
                    st.rerun()
            except Exception as e:
                st.error(f"è®€å–å¤±æ•—: {e}")

st.title("ğŸ§¬ å­¸è¡“é›·é” V12.9 (Future Proof)")
st.caption("æ ¸å¿ƒï¼š**åŒååŒå§“ç¯©é¸** + **Streamlit åƒæ•¸ä¿®æ­£**ã€‚")

# === æ ¸å¿ƒè™•ç†é‚è¼¯ ===
def process_mining(doi_target, action='init'):
    with st.status("æ­£åœ¨å•Ÿå‹• V10.2 ç¶“å…¸å¼•æ“...", expanded=True) as status:
        if action == 'init':
            st.write("ğŸ“¡ æƒæå¼•ç”¨ç¶²çµ¡éª¨æ¶...")
            skeleton = fetch_network_skeleton(doi_target)
            if not skeleton:
                status.update(label="âŒ æ‰¾ä¸åˆ°è³‡æ–™", state="error")
                st.error("æ‰¾ä¸åˆ°è³‡æ–™ã€‚")
                return
            st.session_state.skeleton = skeleton
            st.session_state.offsets = {'a': 0, 'd': 0}
            hero_enriched = enrich_segment([skeleton['hero']])[0]
            st.session_state.full_lineage = {'hero': hero_enriched, 'ancestors': [], 'descendants': []}
            st.session_state.chat_history = []
            st.session_state.pi_analysis_result = None
            st.session_state.pi_raw_data = None
            st.session_state.read_only_mode = False
        
        st.write("ğŸ” æ“´å……è©³ç´°è³‡æ–™ (PIã€æ‘˜è¦)...")
        sk = st.session_state.skeleton
        off = st.session_state.offsets
        
        new_a_objs, new_d_objs = [], []
        if action == 'init':
            new_a_objs = sk['all_ancestors'][0:5]
            new_d_objs = sk['all_descendants'][0:5]
            st.session_state.offsets = {'a': 5, 'd': 5}
        elif action == 'older':
            new_a_objs = sk['all_ancestors'][off['a'] : off['a']+5]
            st.session_state.offsets['a'] += 5
        elif action == 'newer':
            new_d_objs = sk['all_descendants'][off['d'] : off['d']+5]
            st.session_state.offsets['d'] += 5
        elif action == 'expand_both':
            new_a_objs = sk['all_ancestors'][off['a'] : off['a']+5]
            new_d_objs = sk['all_descendants'][off['d'] : off['d']+5]
            st.session_state.offsets['a'] += 5
            st.session_state.offsets['d'] += 5
            
        enriched_a = enrich_segment(new_a_objs)
        enriched_d = enrich_segment(new_d_objs)
        
        exist_a = len(st.session_state.full_lineage['ancestors'])
        for i, p in enumerate(enriched_a): p['code'] = f"A{exist_a + i + 1}"
        exist_d = len(st.session_state.full_lineage['descendants'])
        for i, p in enumerate(enriched_d): p['code'] = f"D{exist_d + i + 1}"
        
        st.session_state.full_lineage['ancestors'].extend(enriched_a)
        st.session_state.full_lineage['descendants'].extend(enriched_d)
        
        st.write("ğŸ§  AI æ­£åœ¨é€²è¡Œæ·±åº¦æ¨è«–...")
        analysis = generate_deep_analysis_classic(
            st.session_state.full_lineage['hero'],
            st.session_state.full_lineage['ancestors'],
            st.session_state.full_lineage['descendants'],
            api_key, model_name
        )
        st.session_state.deep_dive_result = analysis
        status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
        
        time.sleep(0.5)
        st.rerun()

# === é ç±¤ä»‹é¢ ===
if st.session_state.read_only_mode:
    st.warning("âš ï¸ ç´”é–±è®€æ¨¡å¼ (Read-Only)ã€‚")
    st.markdown('<span class="source-badge">ğŸ“„ Archived Report</span>', unsafe_allow_html=True)
    with st.container():
        st.markdown(f'<div class="report-container">{st.session_state.deep_dive_result}</div>', unsafe_allow_html=True)

else:
    tab_insight, tab_broad = st.tabs(["ğŸ•µï¸â€â™€ï¸ æ·±åº¦æŒ–æ˜ (Deep Dive)", "ğŸ”­ å»£åº¦æœå°‹ (Broad Search)"])

    with tab_insight:
        c1, c2 = st.columns([3, 1])
        with c1:
            doi_input = st.text_input("è¼¸å…¥ DOI æˆ– ç¶²å€", value=st.session_state.pre_fill_doi, key="deep_input")
        with c2:
            st.write("")
            st.write("")
            btn_analyze = st.button("ğŸ” åŸ·è¡Œæ·±æ˜", use_container_width=True)

        if st.session_state.skeleton:
            st.divider()
            st.caption("ğŸ”„ æ“´å±•æœå°‹ç¯„åœ")
            cb1, cb2, cb3 = st.columns([1, 1, 1])
            btn_older = cb1.button("â¬…ï¸ æ‰¾æ›´æ—©ç¥–å…ˆ", use_container_width=True)
            btn_both = cb2.button("â†”ï¸ é›™å‘åŒæ™‚æ“´å±•", use_container_width=True)
            btn_newer = cb3.button("æ‰¾æ›´æ–°å¾Œä»£ â¡ï¸", use_container_width=True)
            
            if btn_older: process_mining(doi_input, 'older')
            if btn_newer: process_mining(doi_input, 'newer')
            if btn_both: process_mining(doi_input, 'expand_both')

        if btn_analyze and doi_input and api_key:
            process_mining(doi_input, 'init')

        if st.session_state.get('deep_dive_result'):
            st.markdown('<span class="source-badge">âœ… V10.2 Logic Report</span>', unsafe_allow_html=True)
            st.markdown(f'<div class="report-container">{st.session_state.deep_dive_result}</div>', unsafe_allow_html=True)
            
            st.divider()
            st.markdown("### ğŸ•µï¸â€â™‚ï¸ PI æ·±åº¦åµæ¢ (Identity Verification)")
            st.caption("åŒååŒå§“æ˜¯è³‡æ–™åº«å¸¸è¦‹éŒ¯èª¤ã€‚è«‹åœ¨ä¸‹æ–¹ **ã€Œé©—æ˜æ­£èº«ã€**ï¼Œå‰”é™¤ä¸å±¬æ–¼è©²ä½œè€…çš„è«–æ–‡ã€‚")
            
            all_papers = st.session_state.full_lineage['ancestors'] + [st.session_state.full_lineage['hero']] + st.session_state.full_lineage['descendants']
            pi_options = {}
            
            for p in all_papers:
                auths = p.get('authors', [])
                if not auths: continue
                safe_title = p.get('title', 'Unknown')[:20] + "..."
                def add_opt(a_obj, role):
                    if a_obj.get('authorId'):
                        lbl = f"[{role}] {a_obj.get('name')} (from {safe_title})"
                        if lbl not in pi_options: pi_options[lbl] = a_obj['authorId']

                add_opt(auths[0], "ç¬¬ä¸€ä½œè€…")
                if len(auths) > 1: add_opt(auths[-1], "æœ€å¾Œä½œè€…")
                if len(auths) >= 3: add_opt(auths[-2], "å€’æ•¸ç¬¬äºŒ")
                if len(auths) >= 4: add_opt(auths[-3], "å€’æ•¸ç¬¬ä¸‰")
            
            col_pi_sel, col_pi_btn = st.columns([3, 1])
            with col_pi_sel:
                selected_pi_label = st.selectbox("1ï¸âƒ£ é¸æ“‡è¦åˆ†æçš„ä½œè€…", options=list(pi_options.keys()))
            
            if st.button("2ï¸âƒ£ è¼‰å…¥è«–æ–‡åˆ—è¡¨ (é©—æ˜æ­£èº«)", use_container_width=True) and selected_pi_label:
                target_author_id = pi_options[selected_pi_label]
                with st.spinner("æ­£åœ¨èª¿é–±å­¸è¡“æª”æ¡ˆ..."):
                    raw_data = fetch_author_profile_no_cache(target_author_id)
                    st.session_state.pi_raw_data = raw_data
                    st.session_state.pi_analysis_result = None

            if st.session_state.pi_raw_data:
                author_name = st.session_state.pi_raw_data.get('name', 'Unknown')
                raw_papers = st.session_state.pi_raw_data.get('papers', [])
                
                st.markdown(f"**{author_name}** çš„é«˜å¼•ç”¨è«–æ–‡åˆ—è¡¨ (å…± {len(raw_papers)} ç¯‡)ï¼š")
                st.info("ğŸ’¡ è«‹å‹¾é¸ **ã€ŒçœŸæ­£å±¬æ–¼é€™ä½ä½œè€…ã€** çš„è«–æ–‡ã€‚è‹¥çœ‹åˆ°é ˜åŸŸä¸ç¬¦çš„ï¼ˆå¦‚åŒååŒå§“ï¼‰ï¼Œè«‹å–æ¶ˆå‹¾é¸ã€‚")
                
                df_papers = pd.DataFrame(raw_papers)
                if not df_papers.empty:
                    df_papers['Select'] = True
                    cols = ['Select', 'title', 'year', 'venue', 'citationCount']
                    valid_cols = [c for c in cols if c in df_papers.columns or c == 'Select']
                    df_papers = df_papers[valid_cols]
                    
                    # [Fix] Replace use_container_width with width='stretch'
                    edited_df = st.data_editor(
                        df_papers, 
                        column_config={
                            "Select": st.column_config.CheckboxColumn("ç´å…¥åˆ†æ", help="å‹¾é¸ä»¥ç´å…¥ AI åˆ†æ", default=True),
                            "title": "è«–æ–‡æ¨™é¡Œ",
                            "year": "å¹´ä»½",
                            "venue": "æœŸåˆŠ/æœƒè­°",
                            "citationCount": "å¼•ç”¨æ•¸"
                        },
                        disabled=["title", "year", "venue", "citationCount"],
                        hide_index=True,
                        width='stretch' 
                    )
                    
                    selected_rows = edited_df[edited_df['Select'] == True]
                    count_sel = len(selected_rows)
                    
                    if st.button(f"3ï¸âƒ£ ç¢ºèª ({count_sel} ç¯‡) ä¸¦åŸ·è¡Œ AI åˆ†æ", type="primary", use_container_width=True) and api_key:
                        if count_sel == 0:
                            st.error("è«‹è‡³å°‘é¸æ“‡ä¸€ç¯‡è«–æ–‡ï¼")
                        else:
                            selected_paper_list = selected_rows.to_dict('records')
                            with st.spinner(f"AI æ­£åœ¨é–±è®€é€™ {count_sel} ç¯‡è«–æ–‡ä¸¦åˆ†æé¢¨æ ¼..."):
                                pi_report = generate_author_analysis(author_name, selected_paper_list, api_key, model_name)
                                st.session_state.pi_analysis_result = pi_report
                else:
                    st.warning("æ­¤ä½œè€…æ²’æœ‰æ‰¾åˆ°ç›¸é—œè«–æ–‡è³‡æ–™ã€‚")

            if st.session_state.pi_analysis_result:
                st.markdown('<div class="pi-box">', unsafe_allow_html=True)
                st.markdown(st.session_state.pi_analysis_result)
                st.markdown('</div>', unsafe_allow_html=True)

            st.divider()
            st.subheader("â³ æŠ€è¡“èˆ‡ä½œè€…æ¼”é€²è¡¨")
            table_data = []
            
            def get_auth_display(p):
                auths = p.get('authors', [])
                if not auths: return "Unknown"
                if len(auths) == 1: return auths[0].get('name')
                if len(auths) == 2: return f"{auths[0].get('name')} & {auths[1].get('name')}"
                first = auths[0].get('name')
                last = auths[-1].get('name')
                last_2 = auths[-2].get('name')
                return f"{first} ... {last_2}, {last}"

            for p in all_papers:
                role = "ğŸŸ¨ ä¸»è§’" if p == st.session_state.full_lineage['hero'] else ("ğŸŸ¦ åŸºçŸ³" if p in st.session_state.full_lineage['ancestors'] else "ğŸŸ© å¾ŒçºŒ")
                tldr_text = (p.get('tldr') or {}).get('text')
                abs_text = p.get('abstract')
                smry = (tldr_text or abs_text or "")[:100]
                table_data.append({
                    "è§’è‰²": role, "ä»£è™Ÿ": p.get('code',''), "å¹´ä»½": p.get('year'), 
                    "é—œéµä½œè€…ç¾¤": get_auth_display(p), "æ¨™é¡Œ": p.get('title'), "æ‘˜è¦é‡é»": smry
                })
            
            # [Fix] Replace use_container_width with width='stretch'
            st.dataframe(pd.DataFrame(table_data), width='stretch', hide_index=True)

            st.subheader("ğŸ’¬ è¿½å•æ­·å²å­¸å®¶")
            user_q = st.text_input("æœ‰ç–‘å•å—ï¼Ÿ", key="chat_input")
            if st.button("é€å‡º") and user_q and api_key:
                with st.spinner("AI æ€è€ƒä¸­..."):
                    ctx = [{"code": p.get('code','Hero'), "title": p.get('title','Unknown'), "year": p.get('year','N/A')} for p in all_papers]
                    ans = ask_historian(user_q, ctx, api_key, model_name)
                    st.session_state.chat_history.append({"q": user_q, "a": ans})
            for chat in reversed(st.session_state.chat_history):
                st.markdown(f"<div class='chat-box'><b>Q: {chat['q']}</b><br>A: {chat['a']}</div>", unsafe_allow_html=True)

            st.markdown("#### ğŸ“š å®Œæ•´æ–‡ç»è©³æƒ…")
            st.markdown('<div class="bib-container">', unsafe_allow_html=True)
            for p in all_papers:
                auth_html = ""
                auths = p.get('authors', [])
                if auths:
                    auth_html += f"<span class='auth-tag-first'>{auths[0].get('name','Unknown')} (1st)</span>"
                    if len(auths) > 1:
                        if len(auths) > 3:
                            auth_html += ", ... "
                            auth_html += f", {auths[-2].get('name')} (2nd Last)"
                        auth_html += f", <span class='auth-tag-last'>{auths[-1].get('name')} (Last)</span>"
                else: auth_html = "Unknown"
                
                st.markdown(f"**[{p.get('code','Hero')}]** {p.get('title','Unknown')} ({p.get('year','N/A')})<br>ğŸ›ï¸ {p.get('venue','N/A')} | ğŸ”— Cited: {p.get('citationCount',0)}<br>ğŸ‘¤ {auth_html}", unsafe_allow_html=True)
                st.markdown("---")
            st.markdown('</div>', unsafe_allow_html=True)
            
            if st.button("ğŸŒ ç”Ÿæˆä¸­/è‹±/æ—¥ ç¸½çµå¡"):
                with st.spinner("ç¿»è­¯ä¸­..."):
                    summary = generate_multilingual_abstract(st.session_state.deep_dive_result, api_key, model_name)
                    st.info("å¤šèªè¨€æ‘˜è¦å¡")
                    st.markdown(summary)

    with tab_broad:
        st.markdown("### ğŸ”­ æŠ€è¡“é—œéµå­—æœå°‹")
        broad_query = st.text_input("è¼¸å…¥é—œéµå­—", key="broad_input")
        limit = st.slider("æœå°‹æ•¸é‡", 5, 20, 10)
        
        if st.button("ğŸš€ æœå°‹", key="btn_broad"):
            with st.spinner("æœå°‹ Semantic Scholar è³‡æ–™åº«..."):
                results = search_broad_papers(broad_query, limit)
                if results:
                    json_str = json.dumps(results, indent=2, ensure_ascii=False)
                    st.download_button("ğŸ“¥ ä¸‹è¼‰æœå°‹çµæœåˆ—è¡¨ (JSON)", json_str, "broad_search_results.json", "application/json")
                    
                    st.success(f"æ‰¾åˆ° {len(results)} ç¯‡ç›¸é—œè«–æ–‡")
                    for p in results:
                        with st.container():
                            t_text = (p.get('tldr') or {}).get('text')
                            a_text = p.get('abstract')
                            s_text = (t_text or a_text or "")[:200]
                            st.markdown(f"""
                            <div class="search-card">
                                <div class="sc-title">{p.get('title', 'Unknown')}</div>
                                <div style="font-size:0.9em; color:#616161; margin:5px 0;">ğŸ“… {p.get('year', 'N/A')} | ğŸ›ï¸ {p.get('venue','N/A')} | ğŸ”— Cited: {p.get('citationCount', 0)}</div>
                                <div style="font-size:0.95em; color:#424242;">{s_text}...</div>
                            </div>
                            """, unsafe_allow_html=True)
                            if st.button(f"ğŸ“¥ æ·±åº¦åˆ†æ (ID: {p['paperId']})", key=f"btn_{p['paperId']}"):
                                st.session_state.pre_fill_doi = p['paperId']
                                st.info(f"å·²é¸å®šè«–æ–‡ ID: {p['paperId']}ï¼Œè«‹åˆ‡æ›è‡³ã€Œæ·±åº¦æ´å¯Ÿã€é ç±¤ä¸¦é»æ“ŠåŸ·è¡Œã€‚")
                else:
                    st.warning("æ‰¾ä¸åˆ°ç›¸é—œè«–æ–‡ã€‚")