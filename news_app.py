# ==========================================
# 0. å„ªå…ˆåŸ·è¡Œï¼šè­¦å‘Šå±è”½èˆ‡å¥—ä»¶è¨­å®š
# ==========================================
import warnings
import os
import json
warnings.filterwarnings("ignore")
os.environ["on_bad_lines"] = "skip"

import streamlit as st
import re
import pandas as pd
import time
import requests
import concurrent.futures
import random
from urllib.parse import urlparse
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from datetime import datetime, timedelta
from tenacity import retry, stop_after_attempt, wait_exponential
from tavily import TavilyClient

# ==========================================
# 1. åŸºç¤è¨­å®šèˆ‡ CSSæ¨£å¼
# ==========================================
st.set_page_config(page_title="å…¨åŸŸè§€é»è§£æ V34.3", page_icon="âš¡", layout="wide")

st.markdown("""
<style>
    .stButton button[kind="secondary"] { border: 2px solid #673ab7; color: #673ab7; font-weight: bold; }
    
    .report-paper {
        background-color: #fdfbf7; 
        color: #2c3e50; 
        padding: 40px; 
        border-radius: 4px; 
        margin-bottom: 15px; 
        border: 1px solid #e0e0e0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        font-family: "Microsoft JhengHei", "Georgia", serif;
        line-height: 1.8;
        font-size: 1.05rem;
    }
    
    /* [V34.3] å¼•ç”¨æ¨£å¼å„ªåŒ–ï¼šç°åº•å°å­—ï¼Œé™ä½å¹²æ“¾ */
    .citation {
        font-size: 0.75em;          /* å­—é«”ç¸®å° */
        color: #777777;             /* ç°è‰²æ–‡å­— */
        background-color: #f4f4f4;  /* æ·¡ç°èƒŒæ™¯ */
        padding: 2px 6px;           /* å…§è· */
        border-radius: 4px;         /* åœ“è§’ */
        margin: 0 4px;              /* å·¦å³ç•™ç™½ */
        font-family: sans-serif; 
        border: 1px solid #e0e0e0;  /* æ¥µæ·¡é‚Šæ¡† */
        font-weight: 400;           /* ä¸åŠ ç²— */
        vertical-align: 1px;        /* å¾®èª¿å‚ç›´å°é½Š */
        display: inline-block;      /* ç¢ºä¿æ•´å¡Šé¡¯ç¤º */
    }

    /* é—œéµæ™‚åºå·è»¸è¡¨æ ¼ (HTML Style) */
    .scrollable-table-container {
        height: 600px; 
        overflow-y: auto; 
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        background-color: white;
        margin-bottom: 20px;
    }
    .custom-table {
        width: 100%;
        border-collapse: collapse;
        font-family: "Microsoft JhengHei", sans-serif;
        font-size: 0.95em;
    }
    .custom-table th {
        position: sticky;
        top: 0;
        background-color: #f1f3f4;
        color: #333;
        font-weight: bold;
        padding: 12px 15px;
        text-align: left;
        border-bottom: 2px solid #ddd;
        z-index: 2;
    }
    .custom-table td {
        padding: 10px 15px;
        border-bottom: 1px solid #f0f0f0;
        vertical-align: middle;
        color: #333;
    }
    .custom-table tr:hover {
        background-color: #f8f9fa;
    }
    .custom-table a {
        color: #1a73e8;
        text-decoration: none;
        font-weight: 500;
        font-size: 1.05em;
    }
    .custom-table a:hover {
        text-decoration: underline;
        color: #1557b0;
    }
    
    .methodology-text {
        font-size: 0.9em;
        line-height: 1.6;
        color: #444;
    }
    .methodology-header {
        font-weight: bold;
        color: #1a237e;
        margin-top: 10px;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. è³‡æ–™åº«èˆ‡å…±ç”¨å¸¸æ•¸ (Strict Domain Lists)
# ==========================================
TAIWAN_WHITELIST = [
    "udn.com", "ltn.com.tw", "chinatimes.com", "cna.com.tw", 
    "storm.mg", "setn.com", "ettoday.net", "tvbs.com.tw", 
    "mirrormedia.mg", "thenewslens.com", "upmedia.mg", 
    "rwnews.tw", "news.pts.org.tw", "ctee.com.tw", "businessweekly.com.tw",
    "news.yahoo.com.tw", "ftvnews.com.tw", "newtalk.tw", "nownews.com", "mygopen.com"
]

INDIE_WHITELIST = [
    "twreporter.org", "theinitium.com", "thenewslens.com", 
    "mindiworldnews.com", "vocus.cc", "matters.town", 
    "plainlaw.me", "whogovernstw.org", "rightplus.org", 
    "biosmonthly.com", "storystudio.tw", "womany.net", "dq.yam.com"
]

INTL_WHITELIST = [
    "bbc.com", "cnn.com", "reuters.com", "apnews.com", "bloomberg.com", 
    "wsj.com", "nytimes.com", "dw.com", "voanews.com", "nikkei.com", "nhk.or.jp", "rfi.fr"
]

# åˆ†é¡å°ç…§è¡¨ (ç”¨æ–¼å‰ç«¯é¡¯ç¤º Emoji)
DB_MAP = {
    "CHINA": ["xinhuanet", "people.com.cn", "huanqiu", "cctv", "chinadaily", "taiwan.cn", "gwytb", "guancha"],
    "GREEN": ["ltn", "ftv", "setn", "rti.org", "newtalk", "mirrormedia", "dpp.org", "libertytimes"],
    "BLUE": ["udn", "chinatimes", "tvbs", "cti", "nownews", "ctee", "kmt.org", "uniteddaily"],
    "OFFICIAL": ["cna.com", "pts.org", "mnd.gov", "mac.gov", "tfc-taiwan", "gov.tw"],
    "INDIE": ["twreporter", "theinitium", "thenewslens", "upmedia", "storm.mg", "mindiworld", "vocus", "matters", "plainlaw"],
    "INTL": ["bbc", "cnn", "reuters", "apnews", "bloomberg", "wsj", "nytimes", "dw.com", "voanews", "rfi.fr"],
    "FARM": ["kknews", "read01", "ppfocus", "buzzhand", "bomb01", "qiqi", "inf.news", "toutiao"]
}

# é›œè¨Šé»‘åå–®
NOISE_BLACKLIST = [
    "zhihu.com", "baidu.com", "pinterest.com", "instagram.com", 
    "facebook.com", "tiktok.com", "youtube.com", "dcard.tw", "ptt.cc"
]

def get_domain_name(url):
    try: return urlparse(url).netloc.replace("www.", "")
    except: return ""

def classify_source(url):
    if not url or url == "#": return "OTHER"
    try:
        domain = urlparse(url).netloc.lower()
        clean_domain = domain.replace("www.", "")
    except: return "OTHER"

    for cat, keywords in DB_MAP.items():
        for kw in keywords:
            if kw in domain:
                return cat
    return "OTHER"

def get_category_meta(cat):
    meta = {
        "CHINA": ("ğŸ‡¨ğŸ‡³ ä¸­åœ‹å®˜åª’", "#d32f2f"),
        "FARM": ("â›” å…§å®¹è¾²å ´", "#ef6c00"),
        "BLUE": ("ğŸ”µ æ³›è—è§€é»", "#1565c0"),
        "GREEN": ("ğŸŸ¢ æ³›ç¶ è§€é»", "#2e7d32"),
        "OFFICIAL": ("âšª å®˜æ–¹/ä¸­ç«‹", "#546e7a"),
        "INDIE": ("ğŸ•µï¸ ç¨ç«‹/æ·±åº¦", "#fbc02d"),
        "INTL": ("ğŸŒ åœ‹éš›åª’é«”", "#f57c00"),
        "VIDEO": ("ğŸŸ£ å½±éŸ³ç¤¾ç¾¤", "#7b1fa2"),
        "OTHER": ("ğŸ“„ å…¶ä»–ä¾†æº", "#9e9e9e")
    }
    return meta.get(cat, ("ğŸ“„ å…¶ä»–ä¾†æº", "#9e9e9e"))

# [V34.3 Fix] è¬èƒ½å¼•ç”¨æ ¼å¼åŒ–å‡½å¼
def format_citation_style(text):
    if not text: return ""
    
    def replacement(match):
        # æå–æ‹¬è™Ÿå…§çš„æ‰€æœ‰æ•¸å­—
        nums = re.findall(r'\d+', match.group(0))
        if not nums: return match.group(0)
        # å»é‡ä¸¦æ’åº
        unique_nums = sorted(list(set(nums)), key=int)
        # è¿”å› HTML æ ¼å¼
        return f'<span class="citation">Source {", ".join(unique_nums)}</span>'

    # 1. æ•æ‰é€£çºŒå–®ä¸€å¼•ç”¨: [Source 1][Source 2]
    text = re.sub(r'(\[Source \d+\](?:[,;]?\s*\[Source \d+\])*)', replacement, text)
    
    # 2. æ•æ‰åˆä½µå¼•ç”¨ (å«å…¨å½¢/åŠå½¢æ‹¬è™Ÿ): (Source 1, 2), ï¼ˆSource 1, 2ï¼‰, [Source 1, 2]
    # æ­£å‰‡è§£é‡‹: [\[\(ï¼ˆ] åŒ¹é…ä»»æ„å·¦æ‹¬è™Ÿ, \s*Source\s+ åŒ¹é… Source å­—æ¨£, [\d,ï¼Œã€\s]+ åŒ¹é…æ•¸å­—èˆ‡åˆ†éš”ç¬¦
    text = re.sub(r'([\[\(ï¼ˆ]\s*Source\s+[\d,ï¼Œã€\s]+[\]\)ï¼‰])', replacement, text)
    
    return text

# ç¶²å€æ—¥æœŸæå–å™¨
def extract_date_from_url(url):
    if not url: return None
    patterns = [
        r'/(\d{4})[-/](\d{2})[-/](\d{2})/',
        r'/(\d{4})(\d{2})(\d{2})/',
        r'-(\d{4})(\d{2})(\d{2})'
    ]
    for p in patterns:
        match = re.search(p, url)
        if match:
            y, m, d = match.groups()
            return f"{y}-{m}-{d}"
    return None

def is_chinese(text):
    return bool(re.search(r'[\u4e00-\u9fff]', text))

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½æ¨¡çµ„
# ==========================================

def search_cofacts(query):
    url = "https://cofacts-api.g0v.tw/graphql"
    graphql_query = """
    query ListArticles($text: String!) {
      ListArticles(filter: {q: $text}, orderBy: [{_score: DESC}], first: 3) {
        edges { node { text articleReplies(status: NORMAL) { reply { text type } } } }
      }
    }
    """
    try:
        response = requests.post(url, json={'query': graphql_query, 'variables': {'text': query}}, timeout=3)
        if response.status_code == 200:
            data = response.json()
            articles = data.get('data', {}).get('ListArticles', {}).get('edges', [])
            result_text = ""
            if articles:
                result_text += "ã€Cofacts æŸ¥æ ¸è³‡æ–™åº«ã€‘\n"
                for i, art in enumerate(articles):
                    node = art.get('node', {})
                    rumor = node.get('text', '')[:50]
                    replies = node.get('articleReplies', [])
                    if replies:
                        r_type = replies[0].get('reply', {}).get('type')
                        result_text += f"- è¬ è¨€: {rumor}... (åˆ¤å®š: {r_type})\n"
            return result_text
    except: return ""
    return ""

# ä¸‰è»Œå¹³è¡Œæœå°‹
def execute_tri_track_search(query, api_key_tavily, search_params, is_strict_mode):
    if search_params['max_results'] <= 20 and not is_strict_mode:
        tavily = TavilyClient(api_key=api_key_tavily)
        return tavily.search(query=query, **search_params).get('results', [])

    queries = [
        f"{query} æ–°è äº‹ä»¶ æ™‚é–“è»¸", 
        f"{query} è©•è«– è§€é» çˆ­è­° åˆ†æ", 
        f"{query} æ‡¶äººåŒ… é‡é» å½±éŸ¿"
    ]
    
    sub_params = search_params.copy()
    sub_params['max_results'] = 20 
    
    all_results = []
    seen_urls = set()
    
    def fetch(q):
        try:
            t = TavilyClient(api_key=api_key_tavily)
            return t.search(query=q, **sub_params).get('results', [])
        except: return []

    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(fetch, q) for q in queries]
        for future in concurrent.futures.as_completed(futures):
            res_list = future.result()
            for item in res_list:
                url = item.get('url')
                if url not in seen_urls:
                    seen_urls.add(url)
                    all_results.append(item)
    
    return all_results

def get_search_context(query, api_key_tavily, days_back, selected_regions, max_results, context_report=None):
    try:
        search_params = {
            "search_depth": "advanced",
            "topic": "general",
            "days": days_back,
            "max_results": max_results,
            "exclude_domains": NOISE_BLACKLIST
        }

        target_domains = []
        is_strict_mode = False
        
        if not isinstance(selected_regions, list): selected_regions = [selected_regions]

        for r in selected_regions:
            if "å°ç£" in r:
                target_domains.extend(TAIWAN_WHITELIST)
                is_strict_mode = True
            if "ç¨ç«‹" in r:
                target_domains.extend(INDIE_WHITELIST)
                is_strict_mode = True
            if "äºæ´²" in r or "æ­æ´²" in r or "ç¾æ´²" in r:
                target_domains.extend(INTL_WHITELIST)
                is_strict_mode = True
        
        if is_strict_mode and target_domains:
            target_domains = list(set(target_domains))
            search_params["include_domains"] = target_domains

        results = execute_tri_track_search(query, api_key_tavily, search_params, is_strict_mode)
        results = results[:max_results]
        
        context_text = ""
        for i, res in enumerate(results):
            title = res.get('title', 'No Title')
            url = res.get('url', '#')
            
            pub_date = res.get('published_date')
            if not pub_date:
                url_date = extract_date_from_url(url)
                if url_date:
                    pub_date = url_date
                else:
                    pub_date = "Missing"
            else:
                pub_date = pub_date[:10]
            
            res['final_date'] = pub_date
            
            content = res.get('content', '')[:3000]
            context_text += f"Source {i+1}: [Date: {pub_date}] [Title: {title}] {content} (URL: {url})\n"
            
        return context_text, results, query, is_strict_mode, len(target_domains)
        
    except Exception as e:
        return f"Error: {str(e)}", [], "Error", False, 0

@retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5), reraise=True)
def call_gemini(system_prompt, user_text, model_name, api_key):
    os.environ["GOOGLE_API_KEY"] = api_key
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.0)
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
    chain = prompt | llm
    return chain.invoke({"input": user_text}).content

# æ·±åº¦æˆ°ç•¥åˆ†æ
def run_strategic_analysis(query, context_text, model_name, api_key, mode="FUSION"):
    today_str = datetime.now().strftime("%Y-%m-%d")
    
    if mode == "FUSION":
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ç¤¾æœƒç§‘å­¸ç ”ç©¶å“¡ã€‚
        
        ã€âš ï¸ æ™‚é–“éŒ¨é»ã€‘ï¼šä»Šå¤©æ˜¯ï¼š{today_str}ã€‚
        
        ã€âš ï¸ æ•¸æ“šçµæ§‹æŒ‡ä»¤ (é‡è¦)ã€‘ï¼š
        åœ¨ç”¢ç”Ÿ [DATA_TIMELINE] æ™‚ï¼Œ**ä¸éœ€è¦** è¼¸å‡ºå®Œæ•´çš„ç¶²å€ (URL)ã€‚
        è«‹è¼¸å‡º **ä¾†æºç·¨è™Ÿ (Source ID)**ï¼Œæ ¼å¼ç‚º `Source X` çš„æ•¸å­— `X`ã€‚
        
        ã€åˆ†ææ–¹æ³•è«–ã€‘ï¼š
        1. **è³‡è¨Šæª¢ç´¢**ï¼šé–±è®€å¤§é‡æ–‡æœ¬ï¼Œè­˜åˆ¥è³‡è¨Šé£½å’Œåº¦ã€‚
        2. **æ¡†æ¶åˆ†æ**ï¼šä¾æ“š Entman (1993) ç†è«–ï¼Œè§£æ§‹ä¸åŒé™£ç‡Ÿçš„æ•˜äº‹æ¡†æ¶ã€‚
        3. **ä¸‰è§’é©—è­‰**ï¼šäº¤å‰æ¯”å°å®˜æ–¹èªªæ³•ã€åª’é«”å ±å°èˆ‡ç¬¬ä¸‰æ–¹æŸ¥æ ¸ã€‚
        
        ã€è¼¸å‡ºæ ¼å¼ (åš´æ ¼éµå®ˆ)ã€‘ï¼š
        ### [DATA_TIMELINE]
        (æ ¼å¼ï¼šYYYY-MM-DD|åª’é«”|æ¨™é¡Œ|Source_ID)
        -> Source_ID è«‹å¡«å¯«æ•´æ•¸ (ä¾‹å¦‚ 1, 5, 20)ã€‚
        -> æ—¥æœŸè¦å‰‡ï¼šè‹¥æ¨™ç¤º [Date: Missing]ï¼Œè«‹å˜—è©¦å¾å…§æ–‡æ¨ç®—ï¼›è‹¥ç„¡æ³•æ¨ç®—ï¼Œè«‹å¡«ã€Œè¿‘æœŸã€ã€‚
        
        ### [REPORT_TEXT]
        (Markdown å ±å‘Š - ç¹é«”ä¸­æ–‡)
        è«‹åŒ…å«ä»¥ä¸‹ç« ç¯€ï¼š
        1. **ğŸ“Š å…¨åŸŸç¾æ³æ‘˜è¦ (Situational Analysis)**
        2. **ğŸ” çˆ­è­°é»äº‹å¯¦æŸ¥æ ¸ (Fact-Check)**
        3. **âš–ï¸ åª’é«”æ¡†æ¶å…‰è­œåˆ†æ (Framing Analysis)**
        4. **ğŸ§  æ·±åº¦è­˜è®€èˆ‡åˆ©ç›Šåˆ†æ (Cui Bono)**
        5. **ğŸ¤” çµæ§‹æ€§åæ€ (Critical Reflection)**
        """
        
    elif mode == "DEEP_SCENARIO":
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼æœªä¾†å­¸ (Futures Studies) çš„æˆ°ç•¥é¡§å•ã€‚
        ã€âš ï¸ æ™‚é–“éŒ¨é»ã€‘ï¼šä»Šå¤©æ˜¯ {today_str}ã€‚
        ã€âš ï¸ æœ€é«˜æŒ‡ä»¤ã€‘ï¼šä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
        
        ã€åˆ†ææ–¹æ³•è«–ã€‘ï¼š
        1. **CLA å±¤æ¬¡åˆ†æ**ï¼šè¡¨è±¡ -> ç³»çµ± -> ä¸–ç•Œè§€ -> ç¥è©±ã€‚
        2. **å¯èƒ½æ€§åœ“éŒ**ï¼šæ¨æ¼”ä¸‰ç¨®æƒ…å¢ƒã€‚

        ã€è¼¸å‡ºæ ¼å¼ã€‘ï¼š
        ### [DATA_TIMELINE]
        (ç•™ç©º)
        
        ### [REPORT_TEXT]
        (Markdown å ±å‘Š - ç¹é«”ä¸­æ–‡)
        1. **ğŸ¯ CLA æ·±åº¦è§£æ§‹ (Causal Layered Analysis)**
           - Litany (è¡¨è±¡)
           - System (ç³»çµ±)
           - Worldview (ä¸–ç•Œè§€)
           - Myth (ç¥è©±)
        2. **ğŸ”® æœªä¾†æƒ…å¢ƒæ¨¡æ“¬ (Scenario Planning)**
           - åŸºæº– / è½‰æŠ˜ / æ¥µç«¯æƒ…å¢ƒ
        3. **ğŸ’¡ ç¶œåˆæˆ°ç•¥å»ºè­°**
        """
    else:
        system_prompt = f"è«‹é‡å° {query} é€²è¡Œåˆ†æã€‚"

    return call_gemini(system_prompt, context_text, model_name, api_key)

def parse_gemini_data(text):
    data = {"timeline": [], "report_text": ""}
    
    if not text: return data

    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        
        if "|" in line and len(line.split("|")) >= 3 and (line[0].isdigit() or "20" in line or "Future" in line or "è¿‘æœŸ" in line):
            parts = line.split("|")
            try:
                date = parts[0].strip()
                name = parts[1].strip()
                title = parts[2].strip()
                source_id_str = "0"
                
                if len(parts) >= 4: 
                    raw_id = parts[3].strip()
                    nums = re.findall(r'\d+', raw_id)
                    if nums: source_id_str = nums[0]
                
                if "XX" in date or "xx" in date: date = "è¿‘æœŸ"
                
                data["timeline"].append({
                    "date": date,
                    "media": name,
                    "title": title,
                    "source_id": int(source_id_str)
                })
            except: pass

    if "### [REPORT_TEXT]" in text:
        data["report_text"] = text.split("### [REPORT_TEXT]")[1].strip()
    elif "### REPORT_TEXT" in text:
        data["report_text"] = text.split("### REPORT_TEXT")[1].strip()
    else:
        match = re.search(r"(#+\s*.*æ‘˜è¦|1\.\s*.*æ‘˜è¦|#+\s*.*CLA)", text)
        if match:
            data["report_text"] = text[match.start():]
        else:
            data["report_text"] = text

    return data

def render_html_timeline(timeline_data, sources, blind_mode):
    if not timeline_data:
        return

    table_rows = ""
    for item in timeline_data:
        date = item.get('date', 'è¿‘æœŸ')
        media = "*****" if blind_mode else item.get('media', 'Unknown')
        title = item.get('title', 'No Title')
        
        s_id = item.get('source_id', 0)
        real_url = "#"
        if 0 < s_id <= len(sources):
            real_url = sources[s_id-1].get('url', '#')
            if (date == "è¿‘æœŸ" or "Missing" in date) and 'final_date' in sources[s_id-1]:
                final_d = sources[s_id-1]['final_date']
                if final_d and final_d != "Missing":
                    date = final_d
        
        cat = classify_source(real_url)
        label, _ = get_category_meta(cat)
        emoji = "âšª"
        if "ä¸­åœ‹" in label: emoji = "ğŸ”´"
        elif "æ³›è—" in label: emoji = "ğŸ”µ"
        elif "æ³›ç¶ " in label: emoji = "ğŸŸ¢"
        elif "å®˜æ–¹" in label: emoji = "âšª"
        elif "ç¨ç«‹" in label: emoji = "ğŸ•µï¸"
        elif "åœ‹éš›" in label: emoji = "ğŸŒ"
        elif "è¾²å ´" in label: emoji = "â›”"
        
        if real_url and real_url != "#":
            title_html = f'<a href="{real_url}" target="_blank">{title}</a>'
        else:
            title_html = title

        media_display = f"{emoji} {media}"
        row_html = f"<tr><td style='white-space:nowrap;'>{date}</td><td style='white-space:nowrap;'>{media_display}</td><td>{title_html}</td></tr>"
        table_rows += row_html

    full_html = f"""
    <div class="scrollable-table-container">
    <table class="custom-table">
    <thead>
    <tr>
    <th style="width:120px;">æ—¥æœŸ</th>
    <th style="width:140px;">åª’é«” (URLåˆ†é¡)</th>
    <th>æ–°èæ¨™é¡Œ (é»æ“Šé–±è®€)</th>
    </tr>
    </thead>
    <tbody>
    {table_rows}
    </tbody>
    </table>
    </div>
    """
    
    st.markdown("### ğŸ“… é—œéµç™¼å±•æ™‚åº")
    st.markdown(full_html, unsafe_allow_html=True)

# 4. ä¸‹è¼‰åŠŸèƒ½
def convert_data_to_json(data):
    import json
    return json.dumps(data, indent=2, ensure_ascii=False)

def convert_data_to_md(data):
    return f"""
# å…¨åŸŸè§€é»åˆ†æå ±å‘Š (Academic Standard)
äº§ç”Ÿæ™‚é–“: {datetime.now()}

## 1. æ·±åº¦åˆ†æ
{data.get('report_text')}

## 2. æ™‚é–“è»¸
{pd.DataFrame(data.get('timeline')).to_markdown(index=False)}
    """

# ==========================================
# 5. UI
# ==========================================
with st.sidebar:
    st.title("å…¨åŸŸè§€é»è§£æ V34.3")
    
    analysis_mode = st.radio(
        "é¸æ“‡åˆ†æå¼•æ“ï¼š",
        options=["å…¨åŸŸæ·±åº¦è§£æ (Fusion)", "æœªä¾†ç™¼å±•æ¨æ¼” (Scenario)"],
        captions=["å­¸è¡“æ¡†æ¶ï¼šæ¡†æ¶åˆ†æ + ä¸‰è§’é©—è­‰", "å­¸è¡“æ¡†æ¶ï¼šCLA å±¤æ¬¡åˆ†æ + æœªä¾†å­¸"],
        index=0
    )
    st.markdown("---")
    
    blind_mode = st.toggle("ğŸ™ˆ ç›²æ¸¬æ¨¡å¼ (éš±è—åª’é«”åç¨±)", value=False)
    
    with st.expander("ğŸ”‘ API è¨­å®š", expanded=True):
        if "GOOGLE_API_KEY" in st.secrets:
            st.success("âœ… Gemini Key Ready")
            google_key = st.secrets["GOOGLE_API_KEY"]
        else:
            google_key = st.text_input("Gemini Key", type="password")

        if "TAVILY_API_KEY" in st.secrets:
            st.success("âœ… Tavily Ready")
            tavily_key = st.secrets["TAVILY_API_KEY"]
        else:
            tavily_key = st.text_input("Tavily Key", type="password")
            
        model_name = st.selectbox(
            "æ¨¡å‹ (Gemini 2.5 Series)", 
            ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite"], 
            index=0,
            help="å»ºè­°ä½¿ç”¨ Pro ç‰ˆä»¥ç²å¾—æœ€ä½³çš„é‚è¼¯æ¨æ¼”èˆ‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚"
        )
        
        search_days = st.number_input(
            "æœå°‹æ™‚é–“ç¯„åœ (å¤©æ•¸)",
            min_value=1,
            max_value=1825,
            value=30,
            step=1,
            help="è«‹è¼¸å…¥æ¬²æœå°‹çš„éå»å¤©æ•¸ï¼Œä¸Šé™ç‚º 1825 å¤© (5å¹´)ã€‚"
        )
        
        max_results = st.slider("æœå°‹ç¯‡æ•¸ä¸Šé™ (Sample Size)", 10, 100, 30, help="å¢åŠ ç¯‡æ•¸å¯é¿å…å°æ¨£æœ¬åèª¤ï¼Œä½†æœƒå¢åŠ åˆ†ææ™‚é–“ã€‚")
        
        selected_regions = st.multiselect(
            "æœå°‹è¦–è§’ (Region) - å¯è¤‡é¸",
            ["ğŸ‡¹ğŸ‡¼ å°ç£ (Taiwan)", "ğŸŒ äºæ´² (Asia)", "ğŸŒ æ­æ´² (Europe)", "ğŸŒ ç¾æ´² (Americas)", "ğŸ•µï¸ ç¨ç«‹/è‡ªåª’é«” (Indie)"],
            default=["ğŸ‡¹ğŸ‡¼ å°ç£ (Taiwan)"]
        )

    with st.expander("ğŸ§  å­¸è¡“åˆ†ææ–¹æ³•è«– (Research Methodology)", expanded=True):
        st.markdown("""
        <div class="methodology-text">
        <div class="methodology-header">1. è³‡è¨Šæª¢ç´¢èˆ‡æ¨£æœ¬æª¢å®š (Information Retrieval & Sampling)</div>
        æœ¬ç³»çµ±æ¡ç”¨ <b>é–‹æºæƒ…å ± (OSINT)</b> æ¨™æº–é€²è¡Œè³‡æ–™æ¢å‹˜ã€‚
        <ul>
            <li><b>ä¸‰è»Œå¹³è¡Œæœå°‹</b>ï¼šåŒæ™‚é‡å°ã€Œäº‹å¯¦/æ™‚åºã€ã€ã€Œè§€é»/çˆ­è­°ã€ã€ã€Œæ·±åº¦/æ‡¶äººåŒ…ã€ä¸‰æ¢è»Œé“é€²è¡Œæœå°‹ï¼Œç¢ºä¿è³‡è¨Šå®Œæ•´æ€§ã€‚</li>
            <li><b>ç¶²åŸŸåœç±¬</b>ï¼šåš´æ ¼åŸ·è¡Œç™½åå–®æ©Ÿåˆ¶ï¼Œç¢ºä¿è³‡è¨Šä¾†æºå¯é ã€‚</li>
            <li><b>æ™ºæ…§æ—¥æœŸæå–</b>ï¼šçµåˆ API å…ƒæ•¸æ“šã€URL è¦å‰‡èˆ‡ AI å…§æ–‡æ¨æ–·ï¼Œæœ€å¤§åŒ–é‚„åŸäº‹ä»¶æ™‚é–“ã€‚</li>
        </ul>

        <div class="methodology-header">2. æ¡†æ¶åˆ†æèˆ‡ç«‹å ´åˆ¤å®š (Framing & Stance)</div>
        æœ¬ç ”ç©¶æ¡ç”¨ <b>Entman (1993) çš„æ¡†æ¶ç†è«– (Framing Theory)</b> èˆ‡ <b>æ‰¹åˆ¤è©±èªåˆ†æ (CDA)</b>ã€‚
        <ul>
            <li><b>èªæ„å±¤æ¬¡</b>ï¼šåˆ†ææ–‡æœ¬ä¸­çš„ä¿®è¾­ (Rhetoric)ã€éš±å–» (Metaphor) èˆ‡æ¨™ç±¤åŒ– (Labeling) ç­–ç•¥ã€‚</li>
            <li><b>æ©Ÿæ§‹å±¤æ¬¡</b>ï¼šçµåˆåª’é«”æ‰€æœ‰æ¬Šçµæ§‹ (Ownership) èˆ‡éå¾€æ”¿æ²»å‚¾å‘è³‡æ–™åº«ï¼Œé€²è¡Œé›™é‡é©—è­‰ (Triangulation)ã€‚</li>
        </ul>

        <div class="methodology-header">3. å¯ä¿¡åº¦èˆ‡æŸ¥æ ¸ (Verification)</div>
        æ¡ç”¨å²ä¸¹ä½›å¤§å­¸æ­·å²æ•™è‚²ç¾¤ (SHEG) æå€¡ä¹‹ <b>æ°´å¹³é–±è®€æ³• (Lateral Reading)</b>ã€‚
        <ul>
            <li><b>äº¤å‰æ¯”å°</b>ï¼šå°‡åª’é«”å ±å°èˆ‡ <b>Cofacts è¬ è¨€æŸ¥æ ¸è³‡æ–™åº«</b> åŠå®˜æ–¹åŸå§‹æ–‡ä»¶é€²è¡Œæ¯”å°ã€‚</li>
        </ul>

        <div class="methodology-header">4. æˆ°ç•¥æ¨æ¼”æ¨¡å‹ (Futures Framework)</div>
        åƒ…æ‡‰ç”¨æ–¼ã€Œæœªä¾†ç™¼å±•æ¨æ¼”ã€æ¨¡å¼ã€‚
        <ul>
            <li><b>ç¬¬ä¸€æ€§åŸç† (First Principles)</b>ï¼šè§£æ§‹è­°é¡Œè‡³æœ€åŸºç¤çš„ç‰©ç†æˆ–ç¶“æ¿Ÿé™åˆ¶ã€‚</li>
            <li><b>å±¤æ¬¡åˆ†ææ³• (CLA)</b>ï¼šç”±è¡¨è±¡ (Litany) æ·±å…¥è‡³ç³»çµ±çµæ§‹ (System) èˆ‡ç¤¾æœƒç¥è©± (Myth)ã€‚</li>
            <li><b>å¯èƒ½æ€§åœ“éŒ (Cone of Plausibility)</b>ï¼šå€åˆ†åŸºæº–æƒ…å¢ƒ (Probable)ã€è½‰æŠ˜æƒ…å¢ƒ (Plausible) èˆ‡æ¥µç«¯æƒ…å¢ƒ (Possible)ã€‚</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

    with st.expander("ğŸ“š ç›£æ¸¬è³‡æ–™åº«æ¸…å–®", expanded=False):
        for key, domains in DB_MAP.items():
            label, color = get_category_meta(key)
            st.markdown(f"**{label}**")
            st.markdown(f"`{', '.join(domains[:3])}...`")

    with st.expander("ğŸ“‚ åŒ¯å…¥èˆŠæƒ…å ±", expanded=False):
        past_report_input = st.text_area("è²¼ä¸ŠèˆŠå ±å‘Š Markdownï¼š", height=100)
        
    st.markdown("### ğŸ“¥ å ±å‘ŠåŒ¯å‡º")
    if st.session_state.get('result') or st.session_state.get('scenario_result'):
        export_data = st.session_state.get('result').copy()
        if st.session_state.get('scenario_result'):
            export_data['report_text'] += "\n\n# æœªä¾†ç™¼å±•æ¨æ¼”å ±å‘Š\n" + st.session_state.get('scenario_result')['report_text']
            
        st.download_button("ä¸‹è¼‰ JSON", convert_data_to_json(export_data), "report.json", "application/json")
        st.download_button("ä¸‹è¼‰ Markdown", convert_data_to_md(export_data), "report.md", "text/markdown")

st.title(f"{analysis_mode.split(' ')[0]}")
query = st.text_input("è¼¸å…¥è­°é¡Œé—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ç¾åœ‹è¨­å» çˆ­è­°")
search_btn = st.button("ğŸš€ å•Ÿå‹•å…¨åŸŸæƒæ", type="primary")

# åˆå§‹åŒ– session state
if 'result' not in st.session_state: st.session_state.result = None
if 'scenario_result' not in st.session_state: st.session_state.scenario_result = None
if 'sources' not in st.session_state: st.session_state.sources = None

if search_btn and query and google_key and tavily_key:
    st.session_state.result = None
    st.session_state.scenario_result = None
    
    with st.status("ğŸš€ å•Ÿå‹•å…¨åŸŸæƒæå¼•æ“ (V34.3 æ¨£å¼ä¿®å¾©ç‰ˆ)...", expanded=True) as status:
        
        days_label = f"è¿‘ {search_days} å¤©"
        regions_label = ", ".join([r.split(" ")[1] for r in selected_regions])
        
        st.write(f"ğŸ“¡ 1. é€£ç·š Tavily æœå°‹ (è¦–è§’: {regions_label} / æ™‚é–“: {days_label})...")
        st.write(f"   â†³ ç›®æ¨™æ¨£æœ¬æ•¸: {max_results} ç¯‡ (ä¸‰è»Œæœå°‹ + ç¶²åŸŸåœç±¬)")
        
        context_text, sources, actual_query, is_strict_tw, domain_count = get_search_context(query, tavily_key, search_days, selected_regions, max_results, past_report_input)
        
        if is_strict_tw:
            st.write(f"ğŸ›¡ï¸ ç¶²åŸŸåœç±¬å·²å•Ÿå‹•ï¼šå·²é–å®šç›£æ¸¬è³‡æ–™åº«å…§çš„ {domain_count} å€‹æ¬Šå¨ä¾†æºã€‚")
        else:
            st.write("âš ï¸ æœªé¸å®šå€åŸŸï¼ŒåŸ·è¡Œå…¨ç¶²æœå°‹ (å·²æ’é™¤çŸ¥ä¹ç­‰è¾²å ´)ã€‚")
        
        st.session_state.sources = sources
        
        st.write("ğŸ›¡ï¸ 2. æŸ¥è©¢ Cofacts è¬ è¨€è³‡æ–™åº« (API)...")
        cofacts_txt = search_cofacts(query)
        if cofacts_txt: context_text += f"\n{cofacts_txt}\n"
        
        st.write("ğŸ§  3. AI é€²è¡Œæ·±åº¦æˆ°ç•¥åˆ†æ (å­¸è¡“æ¡†æ¶æ‡‰ç”¨ + æ¨£æœ¬æª¢å®š)...")
        
        # é è¨­åŸ·è¡Œ FUSION æ¨¡å¼
        raw_report = run_strategic_analysis(query, context_text, model_name, google_key, mode="FUSION")
        st.session_state.result = parse_gemini_data(raw_report)
            
        status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
        
    st.rerun()

# é¡¯ç¤ºå€åŸŸ
if st.session_state.result:
    data = st.session_state.result
    # [V34.1] å‚³å…¥ sources ä¾› ID æ˜ å°„ä½¿ç”¨
    render_html_timeline(data.get("timeline"), st.session_state.sources, blind_mode)

    # 2. é¡¯ç¤ºç¬¬ä¸€éšæ®µï¼šç¶œåˆæˆ°ç•¥åˆ†æå ±å‘Š
    st.markdown("---")
    st.markdown("### ğŸ“ ç¶œåˆæˆ°ç•¥åˆ†æå ±å‘Š")
    formatted_text = format_citation_style(data.get("report_text", ""))
    st.markdown(f'<div class="report-paper">{formatted_text}</div>', unsafe_allow_html=True)
    
    # [V33.4] è³‡è¨Šæ»¾å‹•æŒ‰éˆ•
    if "æœªä¾†" not in analysis_mode and not st.session_state.scenario_result:
        st.markdown("---")
        if st.button("ğŸš€ å°‡æ­¤çµæœé¤µçµ¦æœªä¾†ç™¼å±•æ¨æ¼” (è³‡è¨Šæ»¾å‹•)", type="secondary"):
            with st.spinner("ğŸ”® æ­£åœ¨è®€å–å‰æ¬¡æƒ…å ±ï¼Œå•Ÿå‹• CLA å±¤æ¬¡åˆ†æèˆ‡æœªä¾†æ¨æ¼”..."):
                current_report = data.get("report_text", "")
                raw_text = run_strategic_analysis(query, current_report, model_name, google_key, mode="DEEP_SCENARIO")
                st.session_state.scenario_result = parse_gemini_data(raw_text) 
                st.rerun()

# [V33.4] é¡¯ç¤ºç¬¬äºŒéšæ®µï¼šæœªä¾†ç™¼å±•æ¨æ¼”å ±å‘Š
if st.session_state.scenario_result:
    st.markdown("---")
    st.markdown("### ğŸ”® æœªä¾†ç™¼å±•æ¨æ¼”å ±å‘Š")
    scenario_data = st.session_state.scenario_result
    formatted_scenario = format_citation_style(scenario_data.get("report_text", ""))
    st.markdown(f'<div class="report-paper">{formatted_scenario}</div>', unsafe_allow_html=True)

if st.session_state.sources:
    st.markdown("---")
    st.markdown("### ğŸ“š å¼•ç”¨æ–‡ç»åˆ—è¡¨")
    md_table = "| ç·¨è™Ÿ | åª’é«”/ç¶²åŸŸ | æ¨™é¡Œæ‘˜è¦ | é€£çµ |\n|:---:|:---|:---|:---|\n"
    for i, s in enumerate(st.session_state.sources):
        domain = get_domain_name(s.get('url'))
        if blind_mode: domain = "*****"
        
        title = s.get('title', 'No Title')
        if len(title) > 60: title = title[:60] + "..."
        url = s.get('url')
        md_table += f"| **{i+1}** | `{domain}` | {title} | [é»æ“Š]({url}) |\n"
    st.markdown(md_table)
