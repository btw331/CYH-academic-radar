# ==========================================
# 0. å„ªå…ˆåŸ·è¡Œï¼šè­¦å‘Šå±è”½èˆ‡å¥—ä»¶è¨­å®š
# ==========================================
import warnings
import os
import json
warnings.filterwarnings("ignore")
os.environ["on_bad_lines"] = "skip"

import streamlit as st
import re
import pandas as pd
import time
import requests
import concurrent.futures
import random
from urllib.parse import urlparse
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from datetime import datetime
from tenacity import retry, stop_after_attempt, wait_exponential
from tavily import TavilyClient

# ==========================================
# 1. åŸºç¤è¨­å®šèˆ‡ CSSæ¨£å¼
# ==========================================
st.set_page_config(page_title="å…¨åŸŸè§€é»è§£æ V27.4", page_icon="âš–ï¸", layout="wide")

st.markdown("""
<style>
    /* V-Legacy ç¶“å…¸æŒ‡æ¨™å¡ç‰‡ */
    .metric-container {
        text-align: center;
        padding: 15px;
        background-color: #ffffff;
        border-radius: 8px;
        border: 1px solid #f0f0f0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        transition: transform 0.2s;
        margin-bottom: 10px;
    }
    .metric-container:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }
    .metric-score { font-size: 2.5em; font-weight: 700; margin: 0; line-height: 1.2; }
    .metric-label { font-size: 1.0em; font-weight: 500; margin-top: 5px; color: #666; letter-spacing: 1px; }
    
    .report-paper {
        background-color: #fdfbf7; 
        color: #2c3e50; 
        padding: 30px; 
        border-radius: 4px; 
        margin-top: 20px;
        border: 1px solid #e0e0e0;
        box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        font-family: "Georgia", "Cambria", "Times New Roman", serif;
        line-height: 1.8;
        font-size: 1.05rem;
    }
    
    .citation {
        font-size: 0.85em; color: #757575; background-color: #f0f0f0;
        padding: 2px 6px; border-radius: 4px; margin: 0 2px;
        font-family: sans-serif; border: 1px solid #e0e0e0; font-weight: 500;
    }
    
    /* å·è»¸è¡¨æ ¼æ¨£å¼ */
    .scrollable-table-container {
        height: 500px; 
        overflow-y: auto; 
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        background-color: white;
    }
    .custom-table {
        width: 100%;
        border-collapse: collapse;
        font-family: sans-serif;
        font-size: 0.95em;
    }
    .custom-table th {
        position: sticky;
        top: 0;
        background-color: #f8f9fa;
        color: #444;
        padding: 10px;
        text-align: left;
        border-bottom: 2px solid #ddd;
        z-index: 1;
    }
    .custom-table td {
        padding: 10px;
        border-bottom: 1px solid #eee;
        vertical-align: middle;
        color: #333;
    }
    .custom-table tr:hover {
        background-color: #f5f5f5;
    }
    .custom-table a {
        color: #0366d6;
        text-decoration: none;
        font-weight: 500;
    }
    .custom-table a:hover {
        text-decoration: underline;
    }
    
    .stButton button[kind="secondary"] {
        border: 2px solid #673ab7;
        color: #673ab7;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. è³‡æ–™åº«èˆ‡å…±ç”¨å¸¸æ•¸
# ==========================================
TAIWAN_WHITELIST = [
    "udn.com", "ltn.com.tw", "chinatimes.com", "cna.com.tw", 
    "storm.mg", "setn.com", "ettoday.net", "tvbs.com.tw", 
    "mirrormedia.mg", "thenewslens.com", "upmedia.mg", 
    "rwnews.tw", "news.pts.org.tw", "ctee.com.tw", "businessweekly.com.tw",
    "news.yahoo.com.tw"
]

INDIE_WHITELIST = [
    "twreporter.org", "theinitium.com", "thenewslens.com", 
    "mindiworldnews.com", "vocus.cc", "matters.town", 
    "plainlaw.me", "whogovernstw.org", "rightplus.org", 
    "biosmonthly.com", "storystudio.tw", "womany.net", "dq.yam.com"
]

NAME_KEYWORDS = {
    "CHINA": ["æ–°è¯", "äººæ°‘æ—¥å ±", "ç’°çƒ", "å¤®è¦–", "åœ‹å°è¾¦", "ä¸­è©•", "è§£æ”¾è»", "é™¸åª’", "åŒ—äº¬", "å®‹æ¿¤", "xinhuanet", "huanqiu"],
    "GREEN": ["è‡ªç”±", "ä¸‰ç«‹", "æ°‘è¦–", "æ–°é ­æ®¼", "é¡é€±åˆŠ", "æ°‘é€²é»¨", "è³´æ¸…å¾·", "ç¶ ç‡Ÿ", "ç¨æ´¾", "æŠ—ä¸­ä¿å°", "ltn", "setn", "ftv"],
    "BLUE": ["è¯åˆ", "ä¸­åœ‹æ™‚å ±", "ä¸­æ™‚", "TVBS", "ä¸­å¤©", "å·¥å•†æ™‚å ±", "æ—ºæ—º", "åœ‹æ°‘é»¨", "KMT", "ä¾¯å‹å®œ", "è—ç‡Ÿ", "çµ±æ´¾", "udn", "chinatimes"],
    "FARM": ["ç¶²å‚³", "è¬ è¨€", "çˆ†æ–™", "å…§å®¹è¾²å ´", "PTT", "Dcard", "çˆ†æ–™å…¬ç¤¾"],
    "OFFICIAL": ["ä¸­å¤®ç¤¾", "å…¬è¦–", "cna", "pts", "gov"],
    "VIDEO": ["YouTube", "YouTuber", "ç¶²ç´…", "TikTok", "æŠ–éŸ³", "é¤¨é•·", "ç›´æ’­"]
}

DB_MAP = {
    "CHINA": ["xinhuanet.com", "people.com.cn", "huanqiu.com"],
    "GREEN": ["ltn.com.tw", "ftvnews.com.tw", "setn.com"],
    "BLUE": ["udn.com", "chinatimes.com", "tvbs.com.tw"],
    "OFFICIAL": ["cna.com.tw", "pts.org.tw", "mnd.gov.tw"],
    "INDIE": ["twreporter.org", "theinitium.com", "thenewslens.com"],
    "INTL": ["bbc.com", "cnn.com", "reuters.com"]
}

# ç”¨æ–¼è³‡æ–™åº«æ ¡æ­£ (Database Calibration)
CAMP_KEYWORDS = {
    "GREEN": ["è‡ªç”±", "ä¸‰ç«‹", "æ°‘è¦–", "æ–°é ­æ®¼", "é¡é€±åˆŠ", "æ”¾è¨€", "è³´æ¸…å¾·", "æ°‘é€²é»¨", "é’é³¥", "ä¸­å¤®ç¤¾", "Liberty Times"],
    "BLUE": ["è¯åˆ", "ä¸­æ™‚", "ä¸­åœ‹æ™‚å ±", "TVBS", "ä¸­å¤©", "é¢¨å‚³åª’", "åœ‹æ°‘é»¨", "è—ç‡Ÿ", "èµµå°‘åº·", "United Daily", "China Times"],
    "RED": ["æ–°è¯", "äººæ°‘æ—¥å ±", "ç’°çƒ", "å¤®è¦–", "ä¸­è©•", "å›½å°åŠ", "China Daily"]
}

def get_domain_name(url):
    try: return urlparse(url).netloc.replace("www.", "")
    except: return ""

def classify_media_name(name):
    n = name.lower()
    for cat, keywords in NAME_KEYWORDS.items():
        if any(k in n for k in keywords): return cat
    return "OTHER"

def get_category_meta(cat):
    meta = {
        "CHINA": ("ğŸ‡¨ğŸ‡³ ä¸­åœ‹å®˜åª’", "#d32f2f"),
        "FARM": ("â›” å…§å®¹è¾²å ´", "#ef6c00"),
        "BLUE": ("ğŸ”µ æ³›è—è§€é»", "#1565c0"),
        "GREEN": ("ğŸŸ¢ æ³›ç¶ è§€é»", "#2e7d32"),
        "OFFICIAL": ("âšª å®˜æ–¹/ä¸­ç«‹", "#546e7a"),
        "INDIE": ("ğŸ•µï¸ ç¨ç«‹/æ·±åº¦", "#fbc02d"),
        "INTL": ("ğŸŒ åœ‹éš›åª’é«”", "#f57c00"),
        "VIDEO": ("ğŸŸ£ å½±éŸ³ç¤¾ç¾¤", "#7b1fa2"),
        "OTHER": ("ğŸ“„ å…¶ä»–ä¾†æº", "#9e9e9e")
    }
    return meta.get(cat, ("ğŸ“„ å…¶ä»–", "#9e9e9e"))

def get_score_text_color(score):
    if score >= 80: return "#d32f2f"
    if score >= 60: return "#e65100"
    if score >= 40: return "#f57f17"
    if score >= 20: return "#388e3c"
    return "#757575"

def format_citation_style(text):
    if not text: return ""
    def compress_match(match):
        nums = re.findall(r'\d+', match.group(0))
        unique_nums = sorted(list(set(nums)), key=int)
        return f'<span class="citation">Source {",".join(unique_nums)}</span>'
    pattern_compress = r'(\[Source \d+\](?:[,;]?\s*\[Source \d+\])*)'
    text = re.sub(pattern_compress, compress_match, text)
    return text

def is_chinese(text):
    return bool(re.search(r'[\u4e00-\u9fff]', text))

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½æ¨¡çµ„
# ==========================================

def search_cofacts(query):
    url = "https://cofacts-api.g0v.tw/graphql"
    graphql_query = """
    query ListArticles($text: String!) {
      ListArticles(filter: {q: $text}, orderBy: [{_score: DESC}], first: 3) {
        edges { node { text articleReplies(status: NORMAL) { reply { text type } } } }
      }
    }
    """
    try:
        response = requests.post(url, json={'query': graphql_query, 'variables': {'text': query}}, timeout=3)
        if response.status_code == 200:
            data = response.json()
            articles = data.get('data', {}).get('ListArticles', {}).get('edges', [])
            result_text = ""
            if articles:
                result_text += "ã€Cofacts æŸ¥æ ¸è³‡æ–™åº«ã€‘\n"
                for i, art in enumerate(articles):
                    node = art.get('node', {})
                    rumor = node.get('text', '')[:50]
                    replies = node.get('articleReplies', [])
                    if replies:
                        r_type = replies[0].get('reply', {}).get('type')
                        result_text += f"- è¬ è¨€: {rumor}... (åˆ¤å®š: {r_type})\n"
            return result_text
    except: return ""
    return ""

def get_search_context(query, api_key_tavily, days_back, selected_regions, max_results, context_report=None):
    try:
        tavily = TavilyClient(api_key=api_key_tavily)
        
        search_params = {
            "search_depth": "advanced",
            "topic": "general",
            "days": days_back,
            "max_results": max_results
        }

        suffixes = []
        target_domains = [] 
        
        has_taiwan = False
        has_indie = False
        has_intl = False
        
        if not isinstance(selected_regions, list): selected_regions = [selected_regions]

        for r in selected_regions:
            if "å°ç£" in r: 
                has_taiwan = True
                suffixes.append("å°ç£ æ–°è" if is_chinese(query) else "Taiwan News")
                target_domains.extend(TAIWAN_WHITELIST)
            
            if "ç¨ç«‹" in r:
                has_indie = True
                suffixes.append("è©•è«– æ·±åº¦å ±å°") 
                target_domains.extend(INDIE_WHITELIST)
                
            if "äºæ´²" in r: has_intl = True; suffixes.append("Asia News")
            if "æ­æ´²" in r: has_intl = True; suffixes.append("Europe News")
            if "ç¾æ´²" in r: has_intl = True; suffixes.append("US Americas News")
        
        if not suffixes: suffixes.append("News")
        
        search_q = f"{query} {' '.join(suffixes)}"
        if context_report: search_q += " analysis"
        
        search_params["query"] = search_q

        if (has_taiwan or has_indie) and not has_intl:
            search_params["include_domains"] = list(set(target_domains))
        else:
            search_params["exclude_domains"] = [
                "daum.net", "naver.com", "tistory.com",
                "espn.com", "bleacherreport.com", "cbssports.com", 
                "pinterest.com", "amazon.com", "tripadvisor.com"
            ]
        
        actual_query = search_params["query"]
        
        response = tavily.search(**search_params)
        results = response.get('results', [])
        context_text = ""
        
        for i, res in enumerate(results):
            title = res.get('title', 'No Title')
            url = res.get('url', '#')
            pub_date = res.get('published_date')
            if pub_date:
                pub_date = pub_date[:10]
            else:
                pub_date = "----" 
            
            content = res.get('content', '')[:1200]
            context_text += f"Source {i+1}: [Date: {pub_date}] [Title: {title}] {content} (URL: {url})\n"
            
        return context_text, results, actual_query, (has_taiwan or has_indie) and not has_intl
        
    except Exception as e:
        return f"Error: {str(e)}", [], "Error"

@retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5), reraise=True)
def call_gemini(system_prompt, user_text, model_name, api_key):
    os.environ["GOOGLE_API_KEY"] = api_key
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.2)
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
    chain = prompt | llm
    return chain.invoke({"input": user_text}).content

def run_strategic_analysis(query, context_text, model_name, api_key, mode="FUSION"):
    if mode == "FUSION":
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½é›†ã€Œæ·±åº¦èª¿æŸ¥è¨˜è€…ã€èˆ‡ã€Œåª’é«”è­˜è®€å°ˆå®¶ã€æ–¼ä¸€èº«çš„æƒ…å ±åˆ†æå¸«ã€‚
        è«‹é‡å°è­°é¡Œã€Œ{query}ã€é€²è¡Œã€å…¨åŸŸæ·±åº¦è§£æã€‘ï¼Œæ•´åˆäº‹å¯¦æŸ¥æ ¸èˆ‡è§€é»åˆ†æã€‚
        
        ã€è©•åˆ†æŒ‡æ¨™ (0-100)ã€‘(è«‹æ ¹æ“š Context å…§å®¹é€²è¡Œé‡åŒ–è©•ä¼°)ï¼š
        1. Attack (å‚³æ’­ç†±åº¦): è¨è«–å¯†åº¦èˆ‡æƒ…ç·’å¼·çƒˆåº¦ã€‚
        2. Division (è§€é»åˆ†æ­§): é™£ç‡Ÿé–“çš„å°ç«‹ç¨‹åº¦ã€‚
        3. Impact (å½±éŸ¿æ½›åŠ›): å°æ”¿ç­–æˆ–ç¤¾æœƒçš„æ½›åœ¨å½±éŸ¿ã€‚
        4. Resilience (è³‡è¨Šé€æ˜): å®˜æ–¹è³‡æ–™èˆ‡æŸ¥æ ¸çš„å®Œæ•´åº¦ã€‚
        *Threat (ç¶œåˆçˆ­è­°æŒ‡æ•¸): ç¶œåˆä¸Šè¿°æŒ‡æ¨™çš„åŠ æ¬Šè©•åˆ†ã€‚

        ã€è¼¸å‡ºæ ¼å¼ (åš´æ ¼éµå®ˆ)ã€‘ï¼š
        ### [DATA_SCORES]
        Threat: [åˆ†æ•¸]
        Attack: [åˆ†æ•¸]
        Impact: [åˆ†æ•¸]
        Division: [åˆ†æ•¸]
        Resilience: [åˆ†æ•¸]
        
        ### [DATA_TIMELINE]
        (æ ¼å¼ï¼šYYYY-MM-DD|åª’é«”|æ¨™é¡Œ|ç«‹å ´(-10~10)|å¯ä¿¡åº¦(0-10)|ç¶²å€) 
        -> **ç¶²å€ (URL)** å¿…é ˆå°æ‡‰åˆ° Context ä¸­çš„ Source Linkï¼Œä¸å¯ç•™ç™½ã€‚
        -> æ—¥æœŸè«‹å¾ Context [Date:...] æå–ã€‚
        
        ### [REPORT_TEXT]
        (Markdown å ±å‘Š - è«‹ä½¿ç”¨ [Source X] å¼•ç”¨ä¾†æº)
        è«‹åŒ…å«ä»¥ä¸‹ç« ç¯€ï¼š
        1. **ğŸ“Š å…¨åŸŸç¾æ³æ‘˜è¦ (Situation)**
        2. **ğŸ” çˆ­è­°é»äº‹å¯¦æŸ¥æ ¸çŸ©é™£ (Fact-Check)**
        3. **âš–ï¸ åª’é«”è§€é»å…‰è­œå°ç…§ (è—/ç¶ /ç´…/ç¨)**
        4. **ğŸ§  æ·±åº¦è­˜è®€èˆ‡åˆ©ç›Šåˆ†æ (Cui Bono)**
        5. **ğŸ¤” é—œéµåæ€**
        """
    else:
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½è³‡æ·±çš„è¶¨å‹¢é æ¸¬åˆ†æå¸«ã€‚è«‹é‡å°ã€Œ{query}ã€é€²è¡Œæˆ°ç•¥æ¨æ¼”ã€‚
        
        ã€åˆ†ææ ¸å¿ƒ (Foresight Framework)ã€‘ï¼š
        1. **ç¬¬ä¸€æ€§åŸç†**ï¼šå‰–æè­°é¡ŒèƒŒå¾Œçš„åº•å±¤é©…å‹•åŠ›ã€‚
        2. **å¯èƒ½æ€§åœ“éŒ**ï¼šæ¨æ¼”ä¸‰ç¨®æœªä¾†ç™¼å±•è·¯å¾‘ã€‚

        ã€è©•åˆ†å®šç¾©ã€‘ï¼š
        1. Attack -> å½±éŸ¿é¡¯è‘—æ€§
        2. Division -> ç™¼å±•ä¸ç¢ºå®šæ€§
        3. Impact -> æ™‚é–“ç·Šè¿«åº¦
        4. Resilience -> ç³»çµ±è¤‡é›œåº¦
        *Threat -> ç¶œåˆå½±éŸ¿åŠ›

        ã€è¼¸å‡ºæ ¼å¼ã€‘ï¼š
        ### [DATA_SCORES]
        Threat: [åˆ†æ•¸]
        Attack: [åˆ†æ•¸]
        Impact: [åˆ†æ•¸]
        Division: [åˆ†æ•¸]
        Resilience: [åˆ†æ•¸]
        
        ### [DATA_TIMELINE]
        (æ ¼å¼ï¼šYYYY-MM-DD|åª’é«”|æ¨™é¡Œ|ç«‹å ´(0)|å¯ä¿¡åº¦(5)|ç¶²å€)
        -> **ç¶²å€ (URL)** å¿…é ˆä¿ç•™ï¼Œä»¥ä¾¿ä½¿ç”¨è€…é»æ“ŠæŸ¥è­‰ã€‚
        
        ### [REPORT_TEXT]
        (Markdown å ±å‘Š)
        1. **ğŸ¯ ç¬¬ä¸€æ€§åŸç†æ‹†è§£ (åº•å±¤é‚è¼¯)**
        2. **ğŸ”® æœªä¾†æƒ…å¢ƒæ¨¡æ“¬ (å¯èƒ½æ€§åœ“éŒ)**
        3. **ğŸ’¡ ç¶œåˆæˆ°ç•¥å»ºè­°**
        """

    return call_gemini(system_prompt, context_text, model_name, api_key)

# å¼·åˆ¶æ ¡æ­£é‚è¼¯
def calibrate_stance(media_name, ai_score):
    name_clean = media_name.replace("æ–°è", "").replace("å ±å°", "").replace("ç¶²", "")
    
    if any(k in name_clean for k in CAMP_KEYWORDS["GREEN"]):
        if ai_score > 0: return ai_score * -1
        if ai_score == 0: return -3
        return ai_score

    if any(k in name_clean for k in CAMP_KEYWORDS["BLUE"] + CAMP_KEYWORDS["RED"]):
        if ai_score < 0: return ai_score * -1
        if ai_score == 0: return 3
        return ai_score
        
    return ai_score

def parse_gemini_data(text):
    data = {"scores": {"Threat":0, "Attack":0, "Impact":0, "Division":0, "Resilience":0}, 
            "timeline": [], "report_text": ""}
    
    if not text: return data

    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        
        for key in data["scores"]:
            if f"{key}:" in line:
                try: 
                    score_match = re.search(r'\d+', line)
                    if score_match: data["scores"][key] = int(score_match.group())
                except: pass
        
        # [V27.4 Fix] Robust Timeline Parsing
        # å…¼å®¹ 4 æ¬„ (èˆŠ) æˆ– 6 æ¬„ (æ–°)
        if "|" in line and len(line.split("|")) >= 4 and not line.startswith("###") and not "YYYY" in line:
            parts = line.split("|")
            try:
                date = parts[0].strip()
                name = parts[1].strip()
                title = parts[2].strip()
                base_stance = 0
                base_cred = 5
                url = "#"
                
                # 6 Columns: Date|Media|Title|Stance|Cred|URL
                if len(parts) >= 6:
                    base_stance = float(parts[3].strip())
                    base_cred = float(parts[4].strip())
                    url = parts[5].strip()
                # 5 Columns: ...|Title|Cred|URL
                elif len(parts) == 5:
                    base_cred = float(parts[3].strip())
                    url = parts[4].strip()
                
                url = url.rstrip(")").rstrip("]").strip()
                final_stance = calibrate_stance(name, base_stance)
                
                data["timeline"].append({
                    "date": date,
                    "media": name,
                    "title": title,
                    "stance": int(final_stance),
                    "credibility": int(base_cred), 
                    "url": url
                })
            except: pass

    if "### [REPORT_TEXT]" in text:
        data["report_text"] = text.split("### [REPORT_TEXT]")[1].strip()
    elif "### REPORT_TEXT" in text:
        data["report_text"] = text.split("### REPORT_TEXT")[1].strip()
    else:
        match = re.search(r"(#+\s*.*æ‘˜è¦|1\.\s*.*æ‘˜è¦|#+\s*.*ç¬¬ä¸€æ€§åŸç†)", text)
        if match:
            data["report_text"] = text[match.start():]
        else:
            data["report_text"] = text

    return data

# [V27.4] æ¸²æŸ“ HTML å·è»¸è¡¨æ ¼ (å®‰å…¨ç‰ˆ)
def render_html_timeline(timeline_data, blind_mode):
    if not timeline_data:
        st.info("ç„¡æ™‚é–“è»¸è³‡æ–™ã€‚")
        return

    table_rows = ""
    for item in timeline_data:
        # [V27.4 Fix] ä½¿ç”¨ .get() é˜²æ­¢ KeyError
        date = item.get('date', 'Unknown')
        media = "*****" if blind_mode else item.get('media', 'Unknown')
        title = item.get('title', 'No Title')
        url = item.get('url', '#')
        stance = item.get('stance', 0)
        cred = item.get('credibility', 5)
        
        # ç‡ˆè™Ÿ
        stance_dot = "âšª"
        if stance < -2: stance_dot = f'<span style="color:#2e7d32; font-weight:bold;">ğŸŸ¢ {stance}</span>'
        elif stance > 2: stance_dot = f'<span style="color:#1565c0; font-weight:bold;">ğŸ”µ +{stance}</span>'
        else: stance_dot = f'<span style="color:#999;">âšª {stance}</span>'
        
        cred_dot = "ğŸ”´"
        if cred >= 8: cred_dot = f'<span style="color:#2e7d32;">ğŸŸ¢ é«˜</span>'
        elif cred >= 5: cred_dot = f'<span style="color:#f9a825;">ğŸŸ¡ ä¸­</span>'
        else: cred_dot = f'<span style="color:#c62828;">ğŸ”´ ä½</span>'
        
        # Link
        if url and url != "#":
            title_html = f'<a href="{url}" target="_blank">{title}</a>'
        else:
            title_html = title

        table_rows += f"""
        <tr>
            <td style="white-space:nowrap;">{date}</td>
            <td style="white-space:nowrap;">{media}</td>
            <td>{title_html}</td>
            <td style="text-align:center;">{stance_dot}</td>
            <td style="text-align:center;">{cred_dot}</td>
        </tr>
        """

    full_html = f"""
    <div class="scrollable-table-container">
        <table class="custom-table">
            <thead>
                <tr>
                    <th style="width:120px;">æ—¥æœŸ</th>
                    <th style="width:100px;">åª’é«”</th>
                    <th>æ–°èæ¨™é¡Œ (é»æ“Šé–±è®€)</th>
                    <th style="width:80px; text-align:center;">ç«‹å ´</th>
                    <th style="width:80px; text-align:center;">å¯ä¿¡åº¦</th>
                </tr>
            </thead>
            <tbody>
                {table_rows}
            </tbody>
        </table>
    </div>
    """
    
    st.markdown("### ğŸ“… é—œéµç™¼å±•æ™‚åº")
    st.markdown(full_html, unsafe_allow_html=True)

# 4. ä¸‹è¼‰åŠŸèƒ½
def convert_data_to_json(data):
    import json
    return json.dumps(data, indent=2, ensure_ascii=False)

def convert_data_to_md(data):
    return f"""
# å…¨åŸŸè§€é»åˆ†æå ±å‘Š
äº§ç”Ÿæ™‚é–“: {datetime.now()}

## 1. æ ¸å¿ƒæŒ‡æ¨™
Threat: {data['scores'].get('Threat', 0)} | Attack: {data['scores'].get('Attack', 0)}

## 2. æ·±åº¦åˆ†æ
{data.get('report_text')}

## 3. æ™‚é–“è»¸
{pd.DataFrame(data.get('timeline')).to_markdown(index=False)}
    """

# ==========================================
# 5. UI
# ==========================================
with st.sidebar:
    st.title("å…¨åŸŸè§€é»è§£æ V27.4")
    
    analysis_mode = st.radio(
        "é¸æ“‡åˆ†æå¼•æ“ï¼š",
        options=["å…¨åŸŸæ·±åº¦è§£æ (Fusion)", "æœªä¾†ç™¼å±•æ¨æ¼” (Scenario)"],
        captions=["å´é‡ï¼šäº‹å¯¦æŸ¥æ ¸ + åˆ©ç›Šåˆ†æ", "å´é‡ï¼šç¬¬ä¸€æ€§åŸç† + å¯èƒ½æ€§åœ“éŒ"],
        index=0
    )
    st.markdown("---")
    
    blind_mode = st.toggle("ğŸ™ˆ ç›²æ¸¬æ¨¡å¼ (éš±è—åª’é«”åç¨±)", value=False)
    
    with st.expander("ğŸ”‘ API è¨­å®š", expanded=True):
        if "GOOGLE_API_KEY" in st.secrets:
            st.success("âœ… Gemini Key Ready")
            google_key = st.secrets["GOOGLE_API_KEY"]
        else:
            google_key = st.text_input("Gemini Key", type="password")

        if "TAVILY_API_KEY" in st.secrets:
            st.success("âœ… Tavily Ready")
            tavily_key = st.secrets["TAVILY_API_KEY"]
        else:
            tavily_key = st.text_input("Tavily Key", type="password")
            
        model_name = st.selectbox("æ¨¡å‹", ["gemini-2.5-flash", "gemini-2.5-flash-lite", "gemini-2.5-pro"], index=0)
        
        search_days = st.selectbox(
            "æœå°‹æ™‚é–“ç¯„åœ",
            options=[3, 7, 14, 30, 90, 1825],
            format_func=lambda x: "ğŸ“… ä¸é™æ™‚é–“ (All Time)" if x == 1825 else f"è¿‘ {x} å¤©",
            index=2
        )
        
        max_results = st.slider("æœå°‹ç¯‡æ•¸ä¸Šé™", 10, 60, 20)
        
        selected_regions = st.multiselect(
            "æœå°‹è¦–è§’ (Region) - å¯è¤‡é¸",
            ["ğŸ‡¹ğŸ‡¼ å°ç£ (Taiwan)", "ğŸŒ äºæ´² (Asia)", "ğŸŒ æ­æ´² (Europe)", "ğŸŒ ç¾æ´² (Americas)", "ğŸ•µï¸ ç¨ç«‹/è‡ªåª’é«” (Indie)"],
            default=["ğŸ‡¹ğŸ‡¼ å°ç£ (Taiwan)"]
        )

    # [V27.4] ğŸ§  è©³ç´°æ–¹æ³•è«–
    with st.expander("ğŸ§  å…¨åŸŸåˆ†ææ–¹æ³•è«–è©³è§£ (Methodology)", expanded=False):
        st.markdown("""
        **1. æœå°‹èˆ‡è³‡æ–™æ¡é›† (Search Strategy)**
        * **æ··åˆæœå°‹ (Hybrid Search)**: çµåˆ Tavily AI æœå°‹å¼•æ“ï¼Œé‡å°ä¸åŒå€åŸŸæ¡å–ä¸åŒç­–ç•¥ã€‚
          - **å°ç£è¦–è§’**: åš´æ ¼ç™½åå–® (åªæœä¸»æµèˆ‡ç¨ç«‹åª’é«”ï¼Œæ’é™¤å…§å®¹è¾²å ´)ã€‚
          - **åœ‹éš›è¦–è§’**: é—œéµå­—é–å®š (å¦‚ "Taiwan News" + "Asia News")ï¼Œä¸¦æ’é™¤åƒåœ¾ç¶²åŸŸã€‚
        * **æ™‚é–“å›æº¯**: æ”¯æ´å¾ã€Œè¿‘3å¤©ã€åˆ°ã€Œè¿‘5å¹´ (1825å¤©)ã€çš„æ­·å²æœå°‹ã€‚
        * **æ—¥æœŸè£œæ•‘**: è‹¥æ–°è metadata ç¼ºæ—¥æœŸï¼ŒAI æœƒé–±è®€å…§æ–‡å‰æ®µ (å¦‚ 'æ˜¨æ—¥', 'é€±ä¸‰') é€²è¡Œæ¨ç®—ã€‚

        **2. æ”¿æ²»ç«‹å ´åˆ¤å®š (Hybrid Stance)**
        * **æ¡ç”¨ã€Œé›™é‡é©—è­‰æ©Ÿåˆ¶ã€**ï¼š
        * **Step A (AI èªæ„)**ï¼šåˆ†ææ¨™é¡Œèˆ‡å…§æ–‡çš„æƒ…ç·’å¼·å¼± (-10~+10)ã€‚
        * **Step B (è³‡æ–™åº«æ ¡æ­£)**ï¼šé‡å°å·²çŸ¥é™£ç‡Ÿåª’é«”é€²è¡Œå¼·åˆ¶æ ¡æ­£ã€‚
          - **ğŸŸ¢ æ³›ç¶ /æ‰¹åˆ¤**: è‡ªç”±ã€ä¸‰ç«‹ã€æ°‘è¦– (å¼·åˆ¶æ­¸é¡ç‚ºè² åˆ†)ã€‚
          - **ğŸ”µ æ³›è—/é«”åˆ¶**: ä¸­æ™‚ã€è¯åˆã€TVBS (å¼·åˆ¶æ­¸é¡ç‚ºæ­£åˆ†)ã€‚
          - **âšª ä¸­ç«‹**: å®˜æ–¹ã€ç¨ç«‹åª’é«” (ä¾æ“šå…§å®¹å®¢è§€æ€§åˆ¤æ–·)ã€‚
        
        **3. å¯ä¿¡åº¦è©•ä¼° (Credibility)**
        * **æ¬Šå¨åº¦ (Authority)**: è€ƒé‡åª’é«”è²è­½ (å¦‚ä¸­å¤®ç¤¾ > å…§å®¹è¾²å ´)ã€‚
        * **å®Œæ•´æ€§ (Completeness)**: æª¢è¦–æ˜¯å¦åŒ…å«æ˜ç¢ºæ¶ˆæ¯ä¾†æºã€æ•¸æ“šä½è­‰ã€‚
        * **æŸ¥æ ¸ (Fact-Check)**: è‡ªå‹•å°ç…§ Cofacts è¬ è¨€è³‡æ–™åº«ã€‚

        **4. æˆ°ç•¥åˆ†ææ¨¡å‹ (Strategic Framework)**
        * **ç¬¬ä¸€æ€§åŸç† (First Principles)**: æ‹†è§£è­°é¡Œçš„æœ€åº•å±¤é©…å‹•åŠ› (å¦‚äººå£ã€åœ°ç·£ã€ç¶“æ¿Ÿ)ã€‚
        * **å¯èƒ½æ€§åœ“éŒ (Cone of Plausibility)**: 
          - **åŸºæº–æƒ…å¢ƒ (Baseline)**: ç¾ç‹€å»¶çºŒã€‚
          - **è½‰æŠ˜æƒ…å¢ƒ (Plausible)**: é—œéµè®Šæ•¸æ”¹è®Šã€‚
          - **æ¥µç«¯æƒ…å¢ƒ (Wild Card)**: é»‘å¤©éµäº‹ä»¶ã€‚
        """)

    with st.expander("ğŸ“š ç›£æ¸¬è³‡æ–™åº«æ¸…å–®", expanded=False):
        for key, domains in DB_MAP.items():
            label, color = get_category_meta(key)
            st.markdown(f"**{label}**")
            st.markdown(f"`{', '.join(domains[:3])}...`")

    with st.expander("ğŸ“‚ åŒ¯å…¥èˆŠæƒ…å ±", expanded=False):
        past_report_input = st.text_area("è²¼ä¸ŠèˆŠå ±å‘Š Markdownï¼š", height=100)
        
    st.markdown("### ğŸ“¥ å ±å‘ŠåŒ¯å‡º")
    if st.session_state.get('result') or st.session_state.get('wargame_result'):
        active_data = st.session_state.get('wargame_result') if "Scenario" in analysis_mode else st.session_state.get('result')
        if active_data:
            st.download_button("ä¸‹è¼‰ JSON", convert_data_to_json(active_data), "report.json", "application/json")
            st.download_button("ä¸‹è¼‰ Markdown", convert_data_to_md(active_data), "report.md", "text/markdown")

st.title(f"{analysis_mode.split(' ')[0]}")
query = st.text_input("è¼¸å…¥è­°é¡Œé—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ç¾åœ‹è¨­å» çˆ­è­°")
search_btn = st.button("ğŸš€ å•Ÿå‹•å…¨åŸŸæƒæ", type="primary")

if 'result' not in st.session_state: st.session_state.result = None
if 'sources' not in st.session_state: st.session_state.sources = None

if search_btn and query and google_key and tavily_key:
    st.session_state.result = None
    
    with st.status("ğŸš€ å•Ÿå‹•å…¨åŸŸæƒæå¼•æ“ (V27.4)...", expanded=True) as status:
        
        days_label = "ä¸é™æ™‚é–“" if search_days == 1825 else f"è¿‘ {search_days} å¤©"
        regions_label = ", ".join([r.split(" ")[1] for r in selected_regions])
        st.write(f"ğŸ“¡ 1. é€£ç·š Tavily æœå°‹ (è¦–è§’: {regions_label} / æ™‚é–“: {days_label})...")
        
        context_text, sources, actual_query, is_strict_tw = get_search_context(query, tavily_key, search_days, selected_regions, max_results, past_report_input)
        st.session_state.sources = sources
        
        st.write("ğŸ›¡ï¸ 2. æŸ¥è©¢ Cofacts è¬ è¨€è³‡æ–™åº« (API)...")
        cofacts_txt = search_cofacts(query)
        if cofacts_txt: context_text += f"\n{cofacts_txt}\n"
        
        st.write("ğŸ§  3. AI é€²è¡Œæ·±åº¦æˆ°ç•¥åˆ†æ...")
        
        mode_code = "V205" if "æœªä¾†" in analysis_mode else "FUSION"
        raw_report = run_strategic_analysis(query, context_text, model_name, google_key, mode=mode_code)
        st.session_state.result = parse_gemini_data(raw_report)
            
        status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
        
    st.rerun()

if st.session_state.result:
    data = st.session_state.result
    scores = data.get("scores", {})
    
    # 1. æŒ‡æ¨™å¡ç‰‡ (V-Legacy éˆé­‚)
    c1, c2, c3, c4 = st.columns(4)
    if "æœªä¾†" in analysis_mode:
        metrics = [("å½±éŸ¿é¡¯è‘—æ€§", scores.get("Attack", 0)), ("ç™¼å±•ä¸ç¢ºå®šæ€§", scores.get("Division", 0)),
                   ("æ™‚é–“ç·Šè¿«åº¦", scores.get("Impact", 0)), ("ç³»çµ±è¤‡é›œåº¦", scores.get("Resilience", 0))]
    else:
        metrics = [("å‚³æ’­ç†±åº¦", scores.get("Attack", 0)), ("è§€é»åˆ†æ­§", scores.get("Division", 0)),
                   ("å½±éŸ¿æ½›åŠ›", scores.get("Impact", 0)), ("è³‡è¨Šé€æ˜", scores.get("Resilience", 0))]
    
    for col, (label, score) in zip([c1, c2, c3, c4], metrics):
        text_color = get_score_text_color(score)
        col.markdown(f"""
        <div class="metric-container">
            <p class="metric-score" style="color: {text_color};">{score}</p>
            <p class="metric-label">{label}</p>
        </div>
        """, unsafe_allow_html=True)

    # 2. æ™‚é–“è»¸ (V27.4 å®‰å…¨ç‰ˆ HTML)
    render_html_timeline(data.get("timeline"), blind_mode)

    # 3. æ·±åº¦å ±å‘Š
    st.markdown("---")
    st.markdown("### ğŸ“ ç¶œåˆæˆ°ç•¥åˆ†æå ±å‘Š")
    formatted_text = format_citation_style(data.get("report_text", ""))
    st.markdown(f'<div class="report-paper">{formatted_text}</div>', unsafe_allow_html=True)
    
    st.markdown("---")
    if "æœªä¾†" not in analysis_mode:
        if st.button("ğŸš€ å°‡æ­¤çµæœé¤µçµ¦æœªä¾†ç™¼å±•æ¨æ¼” (è³‡è¨Šæ»¾å‹•)", type="secondary"):
            pass 

if st.session_state.sources:
    st.markdown("---")
    st.markdown("### ğŸ“š å¼•ç”¨æ–‡ç»åˆ—è¡¨")
    md_table = "| ç·¨è™Ÿ | åª’é«”/ç¶²åŸŸ | æ¨™é¡Œæ‘˜è¦ | é€£çµ |\n|:---:|:---|:---|:---|\n"
    for i, s in enumerate(st.session_state.sources):
        domain = get_domain_name(s.get('url'))
        if blind_mode: domain = "*****"
        
        title = s.get('title', 'No Title')
        if len(title) > 60: title = title[:60] + "..."
        url = s.get('url')
        md_table += f"| **{i+1}** | `{domain}` | {title} | [é»æ“Š]({url}) |\n"
    st.markdown(md_table)
