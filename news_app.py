# ==========================================
# 0. å„ªå…ˆåŸ·è¡Œï¼šè­¦å‘Šå±è”½èˆ‡å¥—ä»¶è¨­å®š
# ==========================================
import warnings
import os
warnings.filterwarnings("ignore")
os.environ["on_bad_lines"] = "skip"

import streamlit as st
import re
import pandas as pd
import time
from urllib.parse import urlparse
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from datetime import datetime
from tenacity import retry, stop_after_attempt, wait_exponential
from tavily import TavilyClient

# ==========================================
# 1. åŸºç¤è¨­å®šèˆ‡ CSSæ¨£å¼
# ==========================================
st.set_page_config(page_title="å…¨åŸŸè§€é»æœå°‹ V22.1", page_icon="âš–ï¸", layout="wide")

st.markdown("""
<style>
    /* èˆŠç‰ˆç¶“å…¸æŒ‡æ¨™å¡ç‰‡ */
    .metric-container {
        text-align: center;
        padding: 15px;
        background-color: #ffffff;
        border-radius: 8px;
        border: 1px solid #f0f0f0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        transition: transform 0.2s;
    }
    .metric-container:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }
    .metric-score { font-size: 2.8em; font-weight: 700; margin: 0; line-height: 1.2;}
    .metric-label { font-size: 1.0em; font-weight: 500; margin-top: 5px; color: #666; letter-spacing: 1px; }
    
    /* æ»¾å‹•æŒ‰éˆ•æ¨£å¼ */
    .stButton button[kind="secondary"] {
        border: 2px solid #673ab7;
        color: #673ab7;
        font-weight: bold;
    }
    
    /* ä¾†æºé€£çµæ¨£å¼ */
    .source-link { 
        color: #1565c0 !important; 
        text-decoration: none; 
        font-weight: bold;
    }
    .source-link:hover {
        text-decoration: underline;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. è³‡æ–™åº«èˆ‡å…±ç”¨å¸¸æ•¸ (æ–°å¢ INDIE)
# ==========================================
# [V22.0] å°ç£ä¸»æµåª’é«”
TAIWAN_WHITELIST = [
    "udn.com", "ltn.com.tw", "chinatimes.com", "cna.com.tw", 
    "setn.com", "ettoday.net", "tvbs.com.tw", "ctee.com.tw", 
    "businessweekly.com.tw", "news.yahoo.com.tw", "mirrormedia.mg"
]

# [V22.1] ç¨ç«‹/è‡ªåª’é«”/æ·±åº¦åª’é«”ç™½åå–®
INDIE_WHITELIST = [
    "twreporter.org",       # å ±å°è€…
    "theinitium.com",       # ç«¯å‚³åª’
    "thenewslens.com",      # é—œéµè©•è«–ç¶²
    "storm.mg",             # é¢¨å‚³åª’ (æ·±åº¦å€)
    "upmedia.mg",           # ä¸Šå ±
    "mindiworldnews.com",   # æ•è¿ªé¸è®€
    "vocus.cc",             # æ–¹æ ¼å­ (è‡ªåª’é«”)
    "matters.town",         # Matters (Web3 è‡ªåª’é«”)
    "plainlaw.me",          # æ³•å¾‹ç™½è©±æ–‡
    "whogovernstw.org",     # èœå¸‚å ´æ”¿æ²»å­¸
    "rightplus.org",        # å¤šå¤šç›Šå–„
    "biosmonthly.com",      # BIOS monthly
    "storystudio.tw",       # æ•…äº‹ StoryStudio
    "womany.net",           # å¥³äººè¿·
    "dq.yam.com"            # åœ°çƒåœ–è¼¯éšŠ
]

# èˆŠç‰ˆåˆ†é¡å°ç…§è¡¨ (ç”¨æ–¼åˆ†é¡æ¨™ç±¤)
DB_MAP = {
    "CHINA": ["xinhuanet.com", "people.com.cn", "huanqiu.com", "cctv.com", "chinadaily.com.cn", "cgtn.com", "taiwan.cn", "gwytb.gov.cn", "guancha.cn", "thepaper.cn", "sina.com.cn", "163.com", "sohu.com", "ifeng.com", "crntt.com", "hk01.com"],
    "JAPAN": ["nhk.or.jp", "asia.nikkei.com", "yomiuri.co.jp", "asahi.com", "japantimes.co.jp", "mainichi.jp", "sankei.com"],
    "INTL": ["reuters.com", "apnews.com", "bloomberg.com", "wsj.com", "ft.com", "economist.com", "bbc.com", "dw.com", "voanews.com", "thediplomat.com", "foreignpolicy.com", "guardian.co.uk", "aljazeera.com", "rfi.fr", "nytimes.com", "cnn.com", "csis.org"],
    "DIGITAL": ["twreporter.org", "theinitium.com", "storm.mg", "upmedia.mg", "mindiworldnews.com", "allsides.com", "ground.news", "thenewslens.com", "readr.tw", "vocus.cc"],
    "OFFICIAL": ["cna.com.tw", "pts.org.tw", "mnd.gov.tw", "indsr.org.tw", "tfc-taiwan.org.tw", "mygopen.com", "cofacts.tw", "mac.gov.tw"],
    "GREEN": ["ltn.com.tw", "ftvnews.com.tw", "setn.com", "rti.org.tw", "newtalk.tw", "peoplenews.tw", "mirrormedia.mg", "dpp.org.tw"],
    "BLUE": ["udn.com", "chinatimes.com", "tvbs.com.tw", "cti.com.tw", "coolloud.org.tw", "nownews.com", "ctee.com.tw", "want-daily.com", "kmt.org.tw"],
    "FARM": ["kknews.cc", "read01.com", "ppfocus.com", "buzzhand.com", "bomb01.com", "qiqi.news", "lackk.com", "mission-tw.com", "hottopic.com", "weibo.com", "xuehua.us", "inf.news", "toutiao.com", "baidu.com", "ptt.cc", "dcard.tw", "mobile01.com"],
    "VIDEO": ["youtube.com", "youtu.be", "tiktok.com", "douyin.com", "bilibili.com", "ixigua.com"],
    "AGGREGATOR": ["yahoo.com", "msn.com", "linetoday.com", "google.com", "ettoday.net"]
}

NAME_KEYWORDS = {
    "CHINA": ["æ–°è¯", "äººæ°‘æ—¥å ±", "ç’°çƒ", "å¤®è¦–", "åœ‹å°è¾¦", "ä¸­è©•", "è§£æ”¾è»", "é™¸åª’", "åŒ—äº¬", "å®‹æ¿¤", "xinhuanet", "huanqiu"],
    "GREEN": ["è‡ªç”±", "ä¸‰ç«‹", "æ°‘è¦–", "æ–°é ­æ®¼", "é¡é€±åˆŠ", "æ°‘é€²é»¨", "è³´æ¸…å¾·", "ç¶ ç‡Ÿ", "ç¨æ´¾", "æŠ—ä¸­ä¿å°", "ltn", "setn", "ftv"],
    "BLUE": ["è¯åˆ", "ä¸­åœ‹æ™‚å ±", "ä¸­æ™‚", "TVBS", "ä¸­å¤©", "å·¥å•†æ™‚å ±", "æ—ºæ—º", "åœ‹æ°‘é»¨", "KMT", "ä¾¯å‹å®œ", "è—ç‡Ÿ", "çµ±æ´¾", "udn", "chinatimes"],
    "FARM": ["ç¶²å‚³", "è¬ è¨€", "çˆ†æ–™", "å…§å®¹è¾²å ´", "PTT", "Dcard", "çˆ†æ–™å…¬ç¤¾"],
    "OFFICIAL": ["ä¸­å¤®ç¤¾", "å…¬è¦–", "cna", "pts", "gov"],
    "VIDEO": ["YouTube", "YouTuber", "ç¶²ç´…", "TikTok", "æŠ–éŸ³", "é¤¨é•·", "ç›´æ’­"]
}

NARRATIVE_MODULES_LIST = [
    "ç–‘ç¾è«–/åœ‹éš›å­¤ç«‹è«–", "æˆ°çˆ­ææ‡¼/å…©å²¸ç·Šå¼µ", "æ–½æ”¿çˆ­è­°/æ²»ç†èƒ½åŠ›", "æ–‡åŒ–èªåŒ/æ°‘æ—æƒ…æ„Ÿ",
    "è»åŠ›æ‡¸æ®Š/æŠ•é™ä¸»ç¾©", "ç¶“æ¿Ÿä¾è³´/æƒ å°æªæ–½", "æ³•å¾‹æˆ°/ä¸»æ¬Šçˆ­è­°", "é«”åˆ¶å„ªè¶Šè«–", "å…§éƒ¨å”åŠ›/æ”¿æ²»æ”»é˜²"
]
NARRATIVE_MODULES_STR = "\n".join([f"{i+1}. {m}" for i, m in enumerate(NARRATIVE_MODULES_LIST)])

def get_domain_name(url):
    try: return urlparse(url).netloc.replace("www.", "")
    except: return ""

def classify_media_name(name):
    n = name.lower()
    for cat, keywords in NAME_KEYWORDS.items():
        if any(k in n for k in keywords): return cat
    return "OTHER"

def get_category_meta(cat):
    meta = {
        "CHINA": ("ğŸ‡¨ğŸ‡³ ä¸­åœ‹å®˜åª’", "#d32f2f"),
        "FARM": ("â›” å…§å®¹è¾²å ´", "#ef6c00"),
        "BLUE": ("ğŸ”µ æ³›è—è§€é»", "#1565c0"),
        "GREEN": ("ğŸŸ¢ æ³›ç¶ è§€é»", "#2e7d32"),
        "OFFICIAL": ("âšª å®˜æ–¹/ä¸­ç«‹", "#546e7a"),
        "AGGREGATOR": ("ğŸŒ å…¥å£ç¶²ç«™", "#607d8b"),
        "JAPAN": ("ğŸ‡¯ğŸ‡µ æ—¥æœ¬è§€é»", "#f57c00"),
        "INTL": ("ğŸŒ åœ‹éš›åª’é«”", "#f57c00"),
        "DIGITAL": ("ğŸŸ¡ æ•¸ä½/ç¶²åª’", "#fbc02d"),
        "VIDEO": ("ğŸŸ£ å½±éŸ³ç¤¾ç¾¤", "#7b1fa2"),
        "OTHER": ("ğŸ“„ å…¶ä»–ä¾†æº", "#9e9e9e")
    }
    return meta.get(cat, ("å…¶ä»–", "#9e9e9e"))

def classify_source(url):
    url_str = url.lower()
    for cat, keywords in DB_MAP.items():
        for kw in keywords:
            if kw in url_str: return cat
    return "OTHER"

def get_score_text_color(score):
    if score >= 80: return "#d32f2f"
    if score >= 60: return "#e65100"
    if score >= 40: return "#f57f17"
    if score >= 20: return "#388e3c"
    return "#757575"

def is_chinese(text):
    return bool(re.search(r'[\u4e00-\u9fff]', text))

# ==========================================
# 3. é›™æ ¸èåˆåˆ†æå¼•æ“ (V22.1 é‚è¼¯)
# ==========================================

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10), reraise=True)
def call_gemini_with_retry(chain, input_data):
    return chain.invoke(input_data)

# [V22.1] æœå°‹é‚è¼¯å‡ç´šï¼šæ”¯æ´ç™½åå–®æ··åˆæ¨¡å¼
def get_search_results(query, api_key_tavily, days_back, selected_regions, max_results):
    try:
        tavily = TavilyClient(api_key=api_key_tavily)
        
        search_params = {
            "search_depth": "advanced",
            "topic": "general",
            "days": days_back,
            "max_results": max_results
        }

        suffixes = []
        target_domains = [] # æ”¶é›†ç™½åå–®ç¶²åŸŸ
        
        # 1. è™•ç†å€åŸŸé¸æ“‡
        has_taiwan = False
        has_indie = False
        has_intl = False
        
        for r in selected_regions:
            if "å°ç£" in r: 
                has_taiwan = True
                suffixes.append("å°ç£ æ–°è" if is_chinese(query) else "Taiwan News")
                target_domains.extend(TAIWAN_WHITELIST)
            
            if "ç¨ç«‹" in r:
                has_indie = True
                # ç¨ç«‹åª’é«”é€šå¸¸ä¹Ÿæ˜¯ä¸­æ–‡ï¼Œä¸éœ€è¦ç‰¹åˆ¥åŠ è‹±æ–‡å¾Œç¶´ï¼Œé™¤éæƒ³æœ global indie
                suffixes.append("è©•è«– æ·±åº¦å ±å°") 
                target_domains.extend(INDIE_WHITELIST)
                
            if "äºæ´²" in r: 
                has_intl = True
                suffixes.append("Asia News")
            if "æ­æ´²" in r: 
                has_intl = True
                suffixes.append("Europe News")
            if "ç¾æ´²" in r: 
                has_intl = True
                suffixes.append("US Americas News")
        
        if not suffixes: suffixes.append("News")
        search_q = f"{query} {' '.join(suffixes)}"
        search_params["query"] = search_q

        # 2. æ±ºå®šæ˜¯å¦å•Ÿç”¨ç™½åå–® (include_domains)
        # é‚è¼¯ï¼šå¦‚æœåªé¸äº†ã€Œå°ç£ã€æˆ–ã€Œç¨ç«‹åª’é«”ã€ï¼ˆæ²’æœ‰é¸åœ‹éš›å€åŸŸï¼‰ï¼Œå‰‡å•Ÿç”¨åš´æ ¼ç™½åå–®
        if (has_taiwan or has_indie) and not has_intl:
            search_params["include_domains"] = list(set(target_domains)) # å»é‡
        else:
            # å¦‚æœæ··é¸äº†åœ‹éš›å€åŸŸï¼Œä¸èƒ½ç”¨ include_domains (æœƒæŠŠåœ‹éš›æ–°èæ¿¾æ‰)
            # æ”¹ç”¨ exclude_domains æ’é™¤åƒåœ¾
            search_params["exclude_domains"] = [
                "daum.net", "naver.com", "tistory.com",
                "espn.com", "bleacherreport.com", "cbssports.com", 
                "pinterest.com", "amazon.com", "tripadvisor.com"
            ]
        
        # åŸ·è¡Œæœå°‹
        response = tavily.search(**search_params)
        results = response.get('results', [])
        
        context_text = ""
        for i, res in enumerate(results):
            pub_date = res.get('published_date', 'Recent')[:10]
            context_text += f"Source {i+1}: [Date: {pub_date}] [Title: {res.get('title')}] {str(res.get('content'))[:2000]} (URL: {res.get('url')})\n"
            
        return context_text, results

    except Exception as e:
        return f"SEARCH_ERROR: {str(e)}", []

def run_fusion_analysis(query, api_key_google, api_key_tavily, model_name, days_back, selected_regions, max_results, mode="FUSION", context_report=None):
    os.environ["GOOGLE_API_KEY"] = api_key_google
    
    # [V22.1] å‘¼å«æ–°çš„æœå°‹é‚è¼¯
    context_text, results = get_search_results(query, api_key_tavily, days_back, selected_regions, max_results)
    
    if context_report and len(context_report) > 50:
        full_context = f"ã€å‰æ¬¡åˆ†æå ±å‘Š (æ­·å²èƒŒæ™¯)ã€‘\n{context_report}\n\nã€æœ¬æ¬¡æœå°‹æƒ…å ±ã€‘\n{context_text}"
        task_instruction = f"ä½ å·²æ”¶åˆ°ä¸€ä»½æ­·å²åˆ†æå ±å‘Šã€‚è«‹ä»¥æ­¤ç‚ºåŸºç¤ï¼Œçµåˆä»Šæ—¥æœ€æ–°æƒ…å ±ï¼Œé€²è¡Œã€Œæ»¾å‹•å¼ã€çš„æœªä¾†æƒ…å¢ƒæ¨¡æ“¬ã€‚"
    else:
        full_context = context_text
        task_instruction = f"è«‹é‡å°è­°é¡Œã€Œ{query}ã€é€²è¡Œã€å…¨åŸŸæ·±åº¦è§£æã€‘ï¼Œæ•´åˆäº‹å¯¦æŸ¥æ ¸èˆ‡è§€é»åˆ†æã€‚"

    if mode == "V205":
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½è³‡æ·±çš„è¶¨å‹¢é æ¸¬åˆ†æå¸«ã€‚{task_instruction}
        
        ã€åˆ†ææ ¸å¿ƒ (Foresight Framework)ã€‘ï¼š
        1. **ç¬¬ä¸€æ€§åŸç† (First Principles)**ï¼šå‰–æè­°é¡ŒèƒŒå¾Œçš„åº•å±¤é©…å‹•åŠ›ã€‚
        2. **å¯èƒ½æ€§åœ“éŒ (Cone of Plausibility)**ï¼šæ¨æ¼”ä¸‰ç¨®æœªä¾†ç™¼å±•è·¯å¾‘ (åŸºæº–ã€è½‰æŠ˜ã€æ¥µç«¯)ã€‚

        ã€è©•åˆ†å®šç¾©ã€‘ï¼š
        1. Attack -> å½±éŸ¿é¡¯è‘—æ€§
        2. Division -> ç™¼å±•ä¸ç¢ºå®šæ€§
        3. Impact -> æ™‚é–“ç·Šè¿«åº¦
        4. Resilience -> ç³»çµ±è¤‡é›œåº¦
        *Threat -> ç¶œåˆå½±éŸ¿åŠ›

        ã€è¼¸å‡ºæ ¼å¼ã€‘ï¼š
        ### [DATA_SCORES]
        Threat: [åˆ†æ•¸]
        Attack: [åˆ†æ•¸]
        Impact: [åˆ†æ•¸]
        Division: [åˆ†æ•¸]
        Resilience: [åˆ†æ•¸]
        
        ### [DATA_TIMELINE]
        (æ ¼å¼ï¼šFuture-Date|é æ¸¬|äº‹ä»¶)
        
        ### [DATA_NARRATIVES]
        (ç¬¬ä¸€æ€§åŸç†,5)

        ### [REPORT_TEXT]
        (Markdown å ±å‘Š)
        # ğŸ¯ ç¬¬ä¸€æ€§åŸç†æ‹†è§£
        # ğŸ”® æœªä¾†æƒ…å¢ƒæ¨¡æ“¬
        # ğŸ’¡ ç¶œåˆå»ºè­°
        """
    else:
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½é›†ã€Œæ·±åº¦èª¿æŸ¥è¨˜è€…ã€èˆ‡ã€Œåª’é«”è­˜è®€å°ˆå®¶ã€æ–¼ä¸€èº«çš„æƒ…å ±åˆ†æå¸«ã€‚
        è«‹é‡å°è­°é¡Œã€Œ{query}ã€é€²è¡Œã€å…¨åŸŸæ·±åº¦è§£æã€‘ã€‚
        
        ã€è©•åˆ†æŒ‡æ¨™ (0-100)ã€‘ï¼š
        1. Attack (å‚³æ’­ç†±åº¦)
        2. Division (è§€é»åˆ†æ­§)
        3. Impact (å½±éŸ¿æ½›åŠ›)
        4. Resilience (è³‡è¨Šé€æ˜)
        *Threat (çˆ­è­°æŒ‡æ•¸)

        ã€è¼¸å‡ºæ ¼å¼ (åš´æ ¼éµå®ˆ)ã€‘ï¼š
        ### [DATA_SCORES]
        Threat: [åˆ†æ•¸]
        Attack: [åˆ†æ•¸]
        Impact: [åˆ†æ•¸]
        Division: [åˆ†æ•¸]
        Resilience: [åˆ†æ•¸]
        
        ### [DATA_TIMELINE]
        (æ ¼å¼ï¼šYYYY-MM-DD|åª’é«”|æ¨™é¡Œ) -> è«‹å‹™å¿…å¾ Context [Date:...] æå–æ—¥æœŸ
        
        ### [DATA_NARRATIVES]
        (æ ¼å¼ï¼šåŠ‡æœ¬åç¨±,å¼·åº¦1-5)
        {NARRATIVE_MODULES_STR}

        ### [REPORT_TEXT]
        (Markdown å ±å‘Š - è«‹ä½¿ç”¨ [Source X] å¼•ç”¨ä¾†æº)
        è«‹åŒ…å«ä»¥ä¸‹ç« ç¯€ï¼š
        1. **ğŸ“Š å…¨åŸŸç¾æ³æ‘˜è¦**
        2. **ğŸ” çˆ­è­°é»äº‹å¯¦æŸ¥æ ¸çŸ©é™£ (Fact-Check)**
        3. **âš–ï¸ åª’é«”è§€é»å…‰è­œå°ç…§ (åŒ…å«ç¨ç«‹/è‡ªåª’é«”è§€é»)**
        4. **ğŸ§  æ·±åº¦è­˜è®€èˆ‡åˆ©ç›Šåˆ†æ (Cui Bono)**
        5. **ğŸ¤” é—œéµåæ€**
        """

    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.1)
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{full_context}")])
    chain = prompt | llm
    response = call_gemini_with_retry(chain, {"full_context": full_context})
    return response.content, results

def parse_gemini_data(text):
    data = {"scores": {"Threat":0, "Attack":0, "Impact":0, "Division":0, "Resilience":0}, 
            "narratives": {}, "timeline": [], "report_text": ""}
    
    if not text or text.startswith("ERROR") or text == "API_LIMIT_ERROR":
        data["report_text"] = text
        return data

    for line in text.split('\n'):
        line = line.strip()
        for key in data["scores"]:
            if f"{key}:" in line:
                try: data["scores"][key] = int(re.search(r'\d+', line).group())
                except: pass
        
        if "|" in line and (line[0].isdigit() or "Future" in line):
            parts = line.split("|")
            if len(parts) >= 3:
                data["timeline"].append({
                    "date": parts[0].strip(),
                    "media": parts[1].strip(),
                    "event": parts[2].strip()
                })

    if "### [REPORT_TEXT]" in text:
        data["report_text"] = text.split("### [REPORT_TEXT]")[1].strip()
    elif "### REPORT_TEXT" in text:
        data["report_text"] = text.split("### REPORT_TEXT")[1].strip()
    else:
        match = re.search(r"(#+\s*.*æ‘˜è¦|1\.\s*.*æ‘˜è¦)", text)
        if match:
            data["report_text"] = text[match.start():]
        else:
            data["report_text"] = text

    return data

def generate_download_content(query, data, sources):
    timeline_str = "| æ—¥æœŸ | åª’é«” | äº‹ä»¶ |\n|---|---|---|\n"
    for item in data.get('timeline', []):
        timeline_str += f"| {item['date']} | {item['media']} | {item['event']} |\n"

    source_str = ""
    if sources:
        for i, s in enumerate(sources):
            source_str += f"{i+1}. [{s.get('content')[:60]}...]({s.get('url')})\n"

    full_content = f"""
# åˆ†æå ±å‘Šï¼š{query}

**åˆ†ææ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M')}

## 1. æ ¸å¿ƒæŒ‡æ¨™è©•ä¼°
* **ç¶œåˆæŒ‡æ•¸**: {data['scores'].get('Threat', 0)}
* **æŒ‡æ¨™ A**: {data['scores'].get('Attack', 0)}
* **æŒ‡æ¨™ B**: {data['scores'].get('Division', 0)}
* **æŒ‡æ¨™ C**: {data['scores'].get('Impact', 0)}
* **æŒ‡æ¨™ D**: {data['scores'].get('Resilience', 0)}

## 2. é—œéµç™¼å±•æ™‚åº
{timeline_str}

## 3. æ·±åº¦åˆ†æå…§å®¹
{data.get('report_text', 'ç„¡å ±å‘Šå…§å®¹')}

## 4. åƒè€ƒæ–‡ç»
{source_str}
"""
    return full_content

# ==========================================
# 4. ä»‹é¢ (UI)
# ==========================================
with st.sidebar:
    st.title("å…¨åŸŸè§€é»æœå°‹ V22.1")
    
    analysis_mode = st.radio(
        "é¸æ“‡åˆ†æå¼•æ“ï¼š",
        options=["å…¨åŸŸæ·±åº¦è§£æ (Fusion)", "æœªä¾†ç™¼å±•æ¨æ¼”"],
        captions=["èåˆï¼šåª’é«”è­˜è®€ + äº‹å¯¦æŸ¥æ ¸ + åˆ©ç›Šåˆ†æ", "æ¨æ¼”ï¼šç¬¬ä¸€æ€§åŸç† + å¯èƒ½æ€§åœ“éŒ"],
        index=0
    )
    st.markdown("---")

    with st.expander("ğŸ“‚ åŒ¯å…¥å‰æ¬¡å ±å‘Š (æŒçºŒè¿½è¹¤ç”¨)", expanded=False):
        past_report_input = st.text_area(
            "è«‹è²¼ä¸Šä¹‹å‰çš„åˆ†æå ±å‘Šå…§å®¹ (Markdown)ï¼š", 
            height=150, 
            placeholder="åœ¨æ­¤è²¼ä¸ŠèˆŠå ±å‘Šï¼Œç³»çµ±å°‡çµåˆä»Šæ—¥æ–°æƒ…å ±é€²è¡Œæ»¾å‹•åˆ†æ..."
        )

    st.markdown("---")
    blind_mode = st.toggle("ğŸ™ˆ ç›²æ¸¬æ¨¡å¼", value=False)
    
    with st.expander("ğŸ”‘ è¨­å®š & åƒæ•¸", expanded=True):
        if "GOOGLE_API_KEY" in st.secrets:
            google_key = st.secrets["GOOGLE_API_KEY"]
            st.success("âœ… Gemini Key Auto-filled")
        else:
            google_key = st.text_input("Gemini Key", value="", type="password")

        if "TAVILY_API_KEY" in st.secrets:
            tavily_key = st.secrets["TAVILY_API_KEY"]
            st.success("âœ… Tavily Key Auto-filled")
        else:
            tavily_key = st.text_input("Tavily Key", value="", type="password")
            
        model_options = ["gemini-2.5-flash", "gemini-2.5-flash-lite", "gemini-2.5-pro"]
        selected_model = st.selectbox("æ¨¡å‹é¸æ“‡", model_options, index=0)
        
        # [V22.0] æ™‚é–“ç¯„åœ (å«ç„¡é™æ™‚é–“)
        search_days = st.selectbox(
            "æœå°‹æ™‚é–“ç¯„åœ",
            options=[3, 7, 14, 30, 90, 1825],
            format_func=lambda x: "ğŸ“… ä¸é™æ™‚é–“ (5å¹´)" if x == 1825 else f"è¿‘ {x} å¤©",
            index=2
        )
        
        # [V22.0] æœå°‹ç¯‡æ•¸ (è‡ªè¨‚)
        max_results = st.slider("æœå°‹ç¯‡æ•¸ä¸Šé™", 20, 100, 20)
        
        # [V22.1] å€åŸŸè¤‡é¸ (å«ç¨ç«‹åª’é«”)
        selected_regions = st.multiselect(
            "æœå°‹è¦–è§’ (Region) - å¯è¤‡é¸",
            ["ğŸ‡¹ğŸ‡¼ å°ç£ (Taiwan)", "ğŸŒ äºæ´² (Asia)", "ğŸŒ æ­æ´² (Europe)", "ğŸŒ ç¾æ´² (Americas)", "ğŸ•µï¸ ç¨ç«‹/è‡ªåª’é«” (Indie)"],
            default=["ğŸ‡¹ğŸ‡¼ å°ç£ (Taiwan)"]
        )

    with st.expander("ğŸ“– è©•åˆ†æŒ‡æ¨™å®šç¾© (å«å…¬å¼)", expanded=False):
        if "æœªä¾†" in analysis_mode:
            st.markdown("""
- **1. å½±éŸ¿é¡¯è‘—æ€§ (Significance)**: è­°é¡Œæ¬Šé‡ + å½±éŸ¿å±¤ç´š
- **2. ç™¼å±•ä¸ç¢ºå®šæ€§ (Uncertainty)**: æœªçŸ¥è®Šæ•¸ / ç¸½è®Šæ•¸
- **3. æ™‚é–“ç·Šè¿«åº¦ (Urgency)**: 1 / (å‰©é¤˜åæ‡‰æ™‚é–“)
- **4. ç³»çµ±è¤‡é›œåº¦ (Complexity)**: åˆ©å®³é—œä¿‚äººæ•¸é‡ * è€¦åˆåº¦
            """)
        else:
            st.markdown("""
- **1. å‚³æ’­ç†±åº¦ (Attack)**: åª’é«”å ±å°é‡ + ç¤¾ç¾¤è²é‡
- **2. è§€é»åˆ†æ­§ (Division)**: é™£ç‡Ÿå°ç«‹åº¦ + æ¨¡ç³Šåº¦
- **3. å½±éŸ¿æ½›åŠ› (Impact)**: å—çœ¾è¦æ¨¡ + æ™‚é–“
- **4. è³‡è¨Šé€æ˜ (Resilience)**: å®˜æ–¹è³‡æ–™ + ç¬¬ä¸‰æ–¹æŸ¥æ ¸
            """)

# ä¸»ç•«é¢
st.title(f"å…¨åŸŸè§€é»æœå°‹ - {analysis_mode.split(' ')[0]}")
query = st.text_input("è¼¸å…¥æ–°èè­°é¡Œ", placeholder="ä¾‹å¦‚ï¼šå·æ™®å°å°è¨€è«–")
search_btn = st.button("ğŸ” å•Ÿå‹•å…¨åŸŸæƒæ", type="primary")

if 'result' not in st.session_state: st.session_state.result = None
if 'sources' not in st.session_state: st.session_state.sources = None
if 'previous_report' not in st.session_state: st.session_state.previous_report = None

if search_btn and query:
    st.session_state.result = None 
    st.session_state.previous_report = None
    
    with st.spinner("ğŸ•µï¸â€â™‚ï¸ æ­£åœ¨é€²è¡Œå…¨ç¶²æœå°‹èˆ‡æ™ºæ…§åˆ†æ..."):
        mode_code = "V205" if "æœªä¾†" in analysis_mode else "FUSION"
        
        report_context = past_report_input if past_report_input.strip() else None
        
        # [V22.1] å‚³éæ–°åƒæ•¸
        raw_text, sources = run_fusion_analysis(
            query, google_key, tavily_key, selected_model, 
            days_back=search_days, 
            selected_regions=selected_regions,
            max_results=max_results,
            mode=mode_code, 
            context_report=report_context
        )
        
        parsed_data = parse_gemini_data(raw_text)
        st.session_state.result = parsed_data
        st.session_state.sources = sources
        st.rerun()

if st.session_state.result:
    data = st.session_state.result
    sources = st.session_state.sources
    
    if data["report_text"].startswith("ERROR") or data["report_text"] == "API_LIMIT_ERROR":
        st.error(f"âš ï¸ ç³»çµ±è¨Šæ¯ï¼š{data['report_text']}")
        if data["report_text"] == "API_LIMIT_ERROR":
            st.info("ğŸ’¡ å»ºè­°ï¼šè«‹ç¨ç­‰ 30 ç§’å¾Œå†è©¦ã€‚")
    else:
        # 1. å½©è‰²æ–‡å­—æŒ‡æ¨™
        scores = data.get("scores", {})
        c1, c2, c3, c4 = st.columns(4)
        
        if "æœªä¾†" in analysis_mode:
            metrics = [("å½±éŸ¿é¡¯è‘—æ€§", scores.get("Attack", 0)), ("ç™¼å±•ä¸ç¢ºå®šæ€§", scores.get("Division", 0)),
                       ("æ™‚é–“ç·Šè¿«åº¦", scores.get("Impact", 0)), ("ç³»çµ±è¤‡é›œåº¦", scores.get("Resilience", 0))]
        else:
            metrics = [("å‚³æ’­ç†±åº¦", scores.get("Attack", 0)), ("è§€é»åˆ†æ­§", scores.get("Division", 0)),
                       ("å½±éŸ¿æ½›åŠ›", scores.get("Impact", 0)), ("è³‡è¨Šé€æ˜", scores.get("Resilience", 0))]
        
        for col, (label, score) in zip([c1, c2, c3, c4], metrics):
            text_color = get_score_text_color(score)
            col.markdown(f"""
            <div class="metric-container">
                <p class="metric-score" style="color: {text_color};">{score}</p>
                <p class="metric-label">{label}</p>
            </div>
            """, unsafe_allow_html=True)
            
        # 2. æ™‚é–“è»¸è¡¨æ ¼ (åŠ å…¥ç‡ˆè™Ÿ)
        st.markdown("---")
        st.subheader("ğŸ“… é—œéµç™¼å±•æ™‚åº")
        if data["timeline"]:
            processed_data = []
            for item in data["timeline"]:
                media_name = item['media']
                # ç›²æ¸¬æ¨¡å¼è™•ç†
                display_media = "*****" if blind_mode else media_name
                
                cat = classify_media_name(media_name)
                emoji = "âšª"
                if cat == "CHINA": emoji = "ğŸ”´"
                elif cat == "BLUE": emoji = "ğŸ”µ"
                elif cat == "GREEN": emoji = "ğŸŸ¢"
                elif cat == "FARM": emoji = "ğŸŸ "
                elif cat == "VIDEO": emoji = "ğŸŸ£"
                elif cat == "INTL": emoji = "ğŸŒ"
                
                processed_data.append({
                    "æ—¥æœŸ": item['date'],
                    "ä¾†æº": f"{emoji} {display_media}",
                    "äº‹ä»¶æ‘˜è¦": item['event']
                })
            
            df = pd.DataFrame(processed_data)
            st.dataframe(df, width=1200, hide_index=True, use_container_width=True)
        else:
            st.info("ç„¡æ™‚é–“è»¸è³‡æ–™ã€‚")

        # 3. ç¶œåˆåˆ†æå ±å‘Š
        st.markdown("---")
        st.subheader("ğŸ“ ç¶œåˆåˆ†æå ±å‘Š")
        
        # è³‡è¨Šæ»¾å‹•æŒ‰éˆ•
        if "æœªä¾†" not in analysis_mode:
            st.info("ğŸ’¡ æˆ°ç•¥å‡ç´šï¼šæ‚¨å¯ä»¥å°‡æ­¤åˆ†æçµæœä½œç‚ºåŸºç¤ï¼Œé€²è¡Œã€Œå¯èƒ½æ€§åœ“éŒã€æ¨æ¼”ã€‚")
            
            def on_roll_click(current_report):
                st.session_state.previous_report = current_report
                
            if st.button("ğŸš€ é€²è¡Œè³‡è¨Šæ»¾å‹•ï¼šå°‡æ­¤çµæœé¤µçµ¦æœªä¾†ç™¼å±•æ¨æ¼”", type="secondary", on_click=on_roll_click, args=(data["report_text"],)):
                with st.spinner("ğŸ”® æ­£åœ¨è®€å–å‰æ¬¡æƒ…å ±ï¼Œå•Ÿå‹•ç¬¬ä¸€æ€§åŸç†æ¨æ¼”..."):
                    raw_text, _ = run_fusion_analysis(
                        query, google_key, tavily_key, selected_model, 
                        days_back=search_days, selected_regions=selected_regions, max_results=max_results,
                        mode="V205", context_report=st.session_state.previous_report
                    )
                    st.session_state.result = parse_gemini_data(raw_text)
                    st.rerun()

        st.markdown(data.get("report_text", "ç„¡åˆ†æå ±å‘Šã€‚"))
        
        # 4. ä¸‹è¼‰
        st.markdown("---")
        download_content = generate_download_content(query, data, sources)
        st.download_button(label="ğŸ“¥ ä¸‹è¼‰å ±å‘Š", data=download_content, file_name="Report.md", type="primary")

        # 5. åƒè€ƒæ–‡ç» (ç´”æ–‡å­—è¡¨æ ¼)
        st.markdown("---")
        st.subheader("ğŸ“š åƒè€ƒæ–‡ç»")
        if sources:
            df_data = []
            for i, s in enumerate(sources):
                domain = get_domain_name(s.get('url'))
                display_domain = "******" if blind_mode else domain
                title = s.get('title', 'No Title') 
                if not title: title = s.get('content', '')[:30] + "..."
                
                df_data.append({"ç·¨è™Ÿ": i+1, "åª’é«”/ç¶²åŸŸ": display_domain, "æ¨™é¡Œæ‘˜è¦": title, "åŸå§‹é€£çµ": s.get('url')})
            
            df = pd.DataFrame(df_data)
            st.dataframe(
                df, 
                column_config={"åŸå§‹é€£çµ": st.column_config.LinkColumn("é»æ“Šå‰å¾€")},
                hide_index=True,
                use_container_width=True
            )
