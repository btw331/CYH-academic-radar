# ==========================================
# 0. å„ªå…ˆåŸ·è¡Œï¼šè­¦å‘Šå±è”½èˆ‡å¥—ä»¶è¨­å®š
# ==========================================
import warnings
import os
import json
warnings.filterwarnings("ignore")
os.environ["on_bad_lines"] = "skip"

import streamlit as st
import re
import pandas as pd
import time
import requests
import concurrent.futures
import random
import markdown
from urllib.parse import urlparse
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from datetime import datetime, timedelta
from tenacity import retry, stop_after_attempt, wait_exponential
from tavily import TavilyClient

# ==========================================
# 1. åŸºç¤è¨­å®šèˆ‡ CSSæ¨£å¼
# ==========================================
st.set_page_config(page_title="å…¨åŸŸè§€é»è§£æ V36.6", page_icon="âš–ï¸", layout="wide")

CSS_STYLE = """
<style>
    body { font-family: "Microsoft JhengHei", "Georgia", sans-serif; line-height: 1.6; color: #333; }
    .stButton button[kind="secondary"] { border: 2px solid #673ab7; color: #673ab7; font-weight: bold; }
    
    .report-paper {
        background-color: #fdfbf7; 
        color: #2c3e50; 
        padding: 40px; 
        border-radius: 4px; 
        margin-bottom: 15px; 
        border: 1px solid #e0e0e0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        font-family: "Microsoft JhengHei", "Georgia", serif;
        line-height: 1.8;
        font-size: 1.05rem;
    }
    
    .citation {
        font-size: 0.75em;          
        color: #777777;             
        background-color: #f4f4f4;  
        padding: 2px 6px;           
        border-radius: 4px;         
        margin: 0 4px;              
        font-family: sans-serif; 
        border: 1px solid #e0e0e0;  
        font-weight: 400;           
        vertical-align: 1px;        
        display: inline-block;      
    }

    .scrollable-table-container {
        height: 600px; 
        overflow-y: auto; 
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        background-color: white;
        margin-bottom: 20px;
    }
    .custom-table {
        width: 100%;
        border-collapse: collapse;
        font-family: "Microsoft JhengHei", sans-serif;
        font-size: 0.95em;
    }
    .custom-table th {
        position: sticky;
        top: 0;
        background-color: #f1f3f4;
        color: #333;
        font-weight: bold;
        padding: 12px 15px;
        text-align: left;
        border-bottom: 2px solid #ddd;
        z-index: 2;
    }
    .custom-table td {
        padding: 10px 15px;
        border-bottom: 1px solid #f0f0f0;
        vertical-align: middle;
        color: #333;
    }
    .custom-table tr:hover {
        background-color: #f8f9fa;
    }
    .custom-table a {
        color: #1a73e8;
        text-decoration: none;
        font-weight: 500;
        font-size: 1.05em;
    }
    .custom-table a:hover {
        text-decoration: underline;
        color: #1557b0;
    }
    
    @media print {
        .scrollable-table-container { height: auto; overflow: visible; }
        body { font-size: 12pt; }
        a { text-decoration: none; color: #000; }
        .report-paper { box-shadow: none; border: none; padding: 0; }
    }
</style>
"""
st.markdown(CSS_STYLE, unsafe_allow_html=True)

# ==========================================
# 2. è³‡æ–™åº«èˆ‡å…±ç”¨å¸¸æ•¸
# ==========================================
BLUE_WHITELIST = ["udn.com", "chinatimes.com", "tvbs.com.tw", "cti.com.tw", "nownews.com", "ctee.com.tw", "storm.mg"]
GREEN_WHITELIST = ["ltn.com.tw", "ftvnews.com.tw", "setn.com", "rti.org.tw", "newtalk.tw", "mirrormedia.mg", "upmedia.mg"]
OFFICIAL_WHITELIST = ["cna.com.tw", "pts.org.tw", "mnd.gov.tw", "mac.gov.tw", "tfc-taiwan.org.tw", "gov.tw"]
FULL_TAIWAN_WHITELIST = BLUE_WHITELIST + GREEN_WHITELIST + OFFICIAL_WHITELIST + ["yahoo.com.tw", "ettoday.net", "businessweekly.com.tw"]

INDIE_WHITELIST = ["twreporter.org", "theinitium.com", "thenewslens.com", "mindiworldnews.com", "vocus.cc", "matters.town", "plainlaw.me"]
INTL_WHITELIST = ["bbc.com", "cnn.com", "reuters.com", "apnews.com", "bloomberg.com", "wsj.com", "nytimes.com", "dw.com", "voanews.com", "nikkei.com", "nhk.or.jp"]
GRAY_WHITELIST = ["ptt.cc", "dcard.tw", "mobile01.com"]

# [V36.6] ç¶²åŸŸ-åç¨± å°ç…§è¡¨ (ç”¨æ–¼é¡¯ç¤ºçœŸå¯¦åª’é«”åç¨±)
DOMAIN_NAME_MAP = {
    "udn.com": "è¯åˆå ±",
    "chinatimes.com": "ä¸­åœ‹æ™‚å ±",
    "tvbs.com.tw": "TVBS",
    "cti.com.tw": "ä¸­å¤©æ–°è",
    "nownews.com": "NOWnews",
    "ctee.com.tw": "å·¥å•†æ™‚å ±",
    "storm.mg": "é¢¨å‚³åª’",
    "ltn.com.tw": "è‡ªç”±æ™‚å ±",
    "ftvnews.com.tw": "æ°‘è¦–æ–°è",
    "setn.com": "ä¸‰ç«‹æ–°è",
    "rti.org.tw": "å¤®å»£",
    "newtalk.tw": "æ–°é ­æ®¼",
    "mirrormedia.mg": "é¡é€±åˆŠ",
    "upmedia.mg": "ä¸Šå ±",
    "cna.com.tw": "ä¸­å¤®ç¤¾",
    "pts.org.tw": "å…¬è¦–",
    "twreporter.org": "å ±å°è€…",
    "theinitium.com": "ç«¯å‚³åª’",
    "thenewslens.com": "é—œéµè©•è«–ç¶²",
    "mindiworldnews.com": "æ•è¿ªé¸è®€",
    "vocus.cc": "æ–¹æ ¼å­",
    "ptt.cc": "PTT",
    "dcard.tw": "Dcard",
    "bbc.com": "BBC",
    "cnn.com": "CNN",
    "reuters.com": "è·¯é€ç¤¾",
    "apnews.com": "ç¾è¯ç¤¾",
    "bloomberg.com": "å½­åš",
    "wsj.com": "è¯çˆ¾è¡—æ—¥å ±",
    "nytimes.com": "ç´ç´„æ™‚å ±"
}

DB_MAP = {
    "CHINA": ["xinhuanet", "people.com.cn", "huanqiu", "cctv", "chinadaily", "taiwan.cn", "gwytb", "guancha"],
    "GREEN": ["ltn", "ftv", "setn", "rti.org", "newtalk", "mirrormedia", "dpp", "upmedia"],
    "BLUE": ["udn", "chinatimes", "tvbs", "cti", "nownews", "ctee", "kmt", "storm"],
    "OFFICIAL": ["cna.com", "pts.org", "mnd.gov", "mac.gov", "tfc-taiwan", "gov.tw"],
    "INDIE": ["twreporter", "theinitium", "thenewslens", "mindiworld", "vocus", "matters", "plainlaw"],
    "INTL": ["bbc", "cnn", "reuters", "apnews", "bloomberg", "wsj", "nytimes", "dw.com", "voanews", "rfi"],
    "FARM": ["kknews", "read01", "ppfocus", "buzzhand", "bomb01", "qiqi", "inf.news", "toutiao"],
    "SOCIAL": ["ptt.cc", "dcard", "mobile01", "facebook", "youtube"]
}

NOISE_BLACKLIST = ["zhihu.com", "baidu.com", "pinterest.com", "instagram.com", "tiktok.com", "tmall.com", "taobao.com", "163.com", "sohu.com"]

def get_domain_name(url):
    try: return urlparse(url).netloc.replace("www.", "")
    except: return ""

def classify_source(url):
    if not url or url == "#": return "OTHER"
    try:
        domain = urlparse(url).netloc.lower()
        clean_domain = domain.replace("www.", "")
    except: return "OTHER"
    for cat, keywords in DB_MAP.items():
        for kw in keywords:
            if kw in domain: return cat
    return "OTHER"

def get_category_meta(cat):
    meta = {
        "CHINA": ("ğŸ‡¨ğŸ‡³ ä¸­åœ‹å®˜åª’", "#d32f2f"),
        "FARM": ("â›” å…§å®¹è¾²å ´", "#ef6c00"),
        "BLUE": ("ğŸ”µ æ³›è—è§€é»", "#1565c0"),
        "GREEN": ("ğŸŸ¢ æ³›ç¶ è§€é»", "#2e7d32"),
        "OFFICIAL": ("âšª å®˜æ–¹/ä¸­ç«‹", "#546e7a"),
        "INDIE": ("ğŸ•µï¸ ç¨ç«‹/æ·±åº¦", "#fbc02d"),
        "INTL": ("ğŸŒ åœ‹éš›åª’é«”", "#f57c00"),
        "VIDEO": ("ğŸŸ£ å½±éŸ³ç¤¾ç¾¤", "#7b1fa2"),
        "SOCIAL": ("âš ï¸ ç¤¾ç¾¤è²é‡", "#607d8b"),
        "OTHER": ("ğŸ“„ å…¶ä»–ä¾†æº", "#9e9e9e")
    }
    return meta.get(cat, ("ğŸ“„ å…¶ä»–ä¾†æº", "#9e9e9e"))

def format_citation_style(text):
    if not text: return ""
    def replacement(match):
        nums = re.findall(r'\d+', match.group(0))
        if not nums: return match.group(0)
        unique_nums = sorted(list(set(nums)), key=int)
        return f'<span class="citation">Source {", ".join(unique_nums)}</span>'
    text = re.sub(r'(\[Source \d+\](?:[,;]?\s*\[Source \d+\])*)', replacement, text)
    text = re.sub(r'([\[\(ï¼ˆ]\s*Source\s+[\d,ï¼Œã€\s]+[\]\)ï¼‰])', replacement, text)
    return text

def extract_date_from_url(url):
    if not url: return None
    patterns = [r'/(\d{4})[-/](\d{2})[-/](\d{2})/', r'/(\d{4})(\d{2})(\d{2})/', r'-(\d{4})(\d{2})(\d{2})']
    for p in patterns:
        match = re.search(p, url)
        if match: return f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
    return None

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½æ¨¡çµ„
# ==========================================

@retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5))
def generate_dynamic_keywords(query, api_key):
    try:
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=api_key, temperature=0.3)
        prompt = f"""
        è«‹é‡å°è­°é¡Œã€Œ{query}ã€ï¼Œç”Ÿæˆ 3 çµ„æœ€å…·æƒ…å ±åƒ¹å€¼çš„æœå°‹é—œéµå­—ï¼Œåˆ†åˆ¥å°æ‡‰ä»¥ä¸‹ä¸‰å€‹ç¶­åº¦ï¼š
        1. [äº‹å¯¦è»Œ]ï¼šé‡å°äº‹ä»¶ç™¼å±•ã€æ™‚é–“è»¸ã€æ–°èå ±å°ã€‚
        2. [è§€é»è»Œ]ï¼šé‡å°çˆ­è­°ã€æ­£åè©•è«–ã€ç¤¾è«–ã€‚
        3. [æ·±åº¦è»Œ]ï¼šé‡å°æ‡¶äººåŒ…ã€å½±éŸ¿åˆ†æã€æ³•è¦ç´°ç¯€ã€‚
        
        è«‹ç›´æ¥è¼¸å‡º 3 å€‹å­—ä¸²ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œä¸è¦æ¨™è™Ÿã€‚
        ç¯„ä¾‹ï¼š"{query} äº‹ä»¶é€²åº¦, {query} æ­£åçˆ­è­°, {query} æ‡¶äººåŒ…é‡é»"
        """
        resp = llm.invoke(prompt).content
        keywords = [k.strip() for k in resp.split(',') if k.strip()]
        return keywords[:3] if len(keywords) >= 3 else [f"{query} æ–°è äº‹ä»¶", f"{query} çˆ­è­° è©•è«–", f"{query} æ‡¶äººåŒ… åˆ†æ"]
    except:
        return [f"{query} æ–°è äº‹ä»¶", f"{query} çˆ­è­° è©•è«–", f"{query} æ‡¶äººåŒ… åˆ†æ"] 

def search_cofacts(query):
    url = "https://cofacts-api.g0v.tw/graphql"
    graphql_query = """query ListArticles($text: String!) { ListArticles(filter: {q: $text}, orderBy: [{_score: DESC}], first: 3) { edges { node { text articleReplies(status: NORMAL) { reply { text type } } } } } }"""
    try:
        response = requests.post(url, json={'query': graphql_query, 'variables': {'text': query}}, timeout=3)
        if response.status_code == 200:
            data = response.json()
            articles = data.get('data', {}).get('ListArticles', {}).get('edges', [])
            result_text = ""
            if articles:
                result_text += "ã€Cofacts æŸ¥æ ¸è³‡æ–™åº«ã€‘\n"
                for i, art in enumerate(articles):
                    node = art.get('node', {})
                    rumor = node.get('text', '')[:50]
                    replies = node.get('articleReplies', [])
                    if replies:
                        r_type = replies[0].get('reply', {}).get('type')
                        result_text += f"- è¬ è¨€: {rumor}... (åˆ¤å®š: {r_type})\n"
            return result_text
    except: return ""
    return ""

def execute_hybrid_search(query, api_key_tavily, search_params, is_strict_mode, dynamic_keywords, selected_regions):
    tavily = TavilyClient(api_key=api_key_tavily)
    all_results = []
    seen_urls = set()
    
    tasks = []
    
    general_domains = []
    if "å°ç£" in str(selected_regions): general_domains.extend(FULL_TAIWAN_WHITELIST)
    if "ç¨ç«‹" in str(selected_regions): general_domains.extend(INDIE_WHITELIST)
    if "äºæ´²" in str(selected_regions): general_domains.extend(INTL_WHITELIST)
    
    general_params = search_params.copy()
    general_params['max_results'] = 10 
    if is_strict_mode and general_domains:
        general_params['include_domains'] = list(set(general_domains))
    
    tasks.append({"name": "General_Main", "query": query, "params": general_params})
    tasks.append({"name": "General_Fact", "query": dynamic_keywords[0], "params": general_params})
    tasks.append({"name": "General_Opn", "query": dynamic_keywords[1], "params": general_params})
    tasks.append({"name": "General_Deep", "query": dynamic_keywords[2], "params": general_params})
    
    if "å°ç£" in str(selected_regions):
        blue_params = search_params.copy()
        blue_params['max_results'] = 5 
        blue_params['include_domains'] = BLUE_WHITELIST
        tasks.append({"name": "Blue_Guard", "query": f"{query}", "params": blue_params})
        
        green_params = search_params.copy()
        green_params['max_results'] = 5 
        green_params['include_domains'] = GREEN_WHITELIST
        tasks.append({"name": "Green_Guard", "query": f"{query}", "params": green_params})
        
        official_params = search_params.copy()
        official_params['max_results'] = 5
        official_params['include_domains'] = OFFICIAL_WHITELIST
        tasks.append({"name": "Official_Guard", "query": f"{query} è²æ˜ æ–°èç¨¿", "params": official_params})

    def fetch(task):
        try:
            return tavily.search(query=task['query'], **task['params']).get('results', [])
        except: return []

    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
        futures = {executor.submit(fetch, t): t['name'] for t in tasks}
        results_map = {}
        for future in concurrent.futures.as_completed(futures):
            t_name = futures[future]
            results_map[t_name] = future.result()
            
    final_list = []
    
    for guard_name in ["Blue_Guard", "Green_Guard", "Official_Guard"]:
        if guard_name in results_map:
            for item in results_map[guard_name]:
                if item['url'] not in seen_urls:
                    seen_urls.add(item['url'])
                    final_list.append(item)
    
    general_keys = ["General_Fact", "General_Opn", "General_Deep", "General_Main"]
    max_len = max([len(results_map.get(k, [])) for k in general_keys]) if general_keys else 0
    
    for i in range(max_len):
        for key in general_keys:
            if key in results_map and i < len(results_map[key]):
                item = results_map[key][i]
                if item['url'] not in seen_urls:
                    seen_urls.add(item['url'])
                    final_list.append(item)
                
    return final_list

def get_search_context(query, api_key_tavily, days_back, selected_regions, max_results, enable_outpost, dynamic_keywords):
    try:
        active_blacklist = [d for d in NOISE_BLACKLIST if d not in ["ptt.cc", "dcard.tw"]] if enable_outpost else NOISE_BLACKLIST

        search_params = {
            "search_depth": "advanced",
            "topic": "general",
            "days": days_back,
            "exclude_domains": active_blacklist
        }

        is_strict_mode = bool(selected_regions)
        results = execute_hybrid_search(query, api_key_tavily, search_params, is_strict_mode, dynamic_keywords, selected_regions)
        
        results.sort(key=lambda x: x.get('published_date') or "", reverse=True)
        results = results[:max_results]
        
        context_text = ""
        for i, res in enumerate(results):
            title = res.get('title', 'No Title')
            url = res.get('url', '#')
            
            pub_date = res.get('published_date')
            if not pub_date:
                url_date = extract_date_from_url(url)
                pub_date = url_date if url_date else "Missing"
            else:
                pub_date = pub_date[:10]
            
            res['final_date'] = pub_date
            content = res.get('content', '')[:3000]
            context_text += f"Source {i+1}: [Date: {pub_date}] [Title: {title}] {content} (URL: {url})\n"
            
        return context_text, results, query, is_strict_mode
        
    except Exception as e:
        return f"Error: {str(e)}", [], "Error", False

@retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5), reraise=True)
def call_gemini(system_prompt, user_text, model_name, api_key):
    os.environ["GOOGLE_API_KEY"] = api_key
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.0)
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
    chain = prompt | llm
    return chain.invoke({"input": user_text}).content

def run_strategic_analysis(query, context_text, model_name, api_key, mode="FUSION"):
    today_str = datetime.now().strftime("%Y-%m-%d")
    
    tone_instruction = """
    ã€âš ï¸ èªæ°£é¢¨æ ¼æŒ‡ä»¤ã€‘ï¼š
    1. **æ¥µåº¦å¯©æ…**ï¼šåš´ç¦è‡†æ¸¬ã€‚è‹¥è­‰æ“šä¸è¶³ï¼Œè«‹ç›´æ¥æ¨™ç¤ºã€Œç›®å‰è³‡è¨Šä¸è¶³ã€ã€‚
    2. **å»è»äº‹åŒ–**ï¼šåš´ç¦ä½¿ç”¨è»äº‹éš±å–»ã€‚
    3. **ä¸­æ€§å°ˆæ¥­**ï¼šä½¿ç”¨ç¤¾æœƒç§‘å­¸è¡“èªã€‚
    """

    if mode == "FUSION":
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„æƒ…å ±åˆ†æå¸«ã€‚
        
        ã€âš ï¸ æ™‚é–“éŒ¨é»ã€‘ï¼šä»Šå¤©æ˜¯ {today_str}ã€‚
        {tone_instruction}
        
        ã€âš ï¸ æ•¸æ“šçµæ§‹æŒ‡ä»¤ã€‘ï¼šè¼¸å‡º Source ID (å¦‚ Source 1)ã€‚
        
        ã€åˆ†ææ–¹æ³•è«–ã€‘ï¼š
        1. **é‚è¼¯è¬¬èª¤åµæ¸¬**ï¼šæŒ‡å‡ºæ»‘å¡è¬¬èª¤ã€ç¨»è‰äººè«–è­‰ã€‚
        2. **è­‰æ“šå¼·åº¦åˆ†ç´š**ï¼šè©•ä¼°è­‰æ“šåŠ›ï¼ˆå¼·/å¼±ï¼‰ã€‚
        3. **è²é‡æ¬Šé‡æ ¡æ­£ (Volume Calibration)**ï¼š
           - **è­˜åˆ¥è¤‡è®€æ©Ÿ**ï¼šè‹¥æŸä¸€é™£ç‡Ÿçš„ä¾†æºå¤§é‡é‡è¤‡ç›¸åŒè§€é»ï¼Œè«‹å°‡å…¶æ­¸ç´ç‚ºã€Œå–®ä¸€å¼·å‹¢è«–é»ã€ã€‚
           - **æŒ–æ˜é•·å°¾**ï¼šå„ªå…ˆå°‹æ‰¾ã€Œéä¸»æµä½†å…·ç¨ç‰¹è¦–è§’ã€çš„è§€é»ã€‚
           - **æ²‰é»˜çš„èºæ—‹**ï¼šè‹¥æŸæ–¹è²é‡é¡¯è‘—ä½è½ï¼Œè«‹æŒ‡å‡ºé€™æ˜¯ã€Œç­–ç•¥æ€§å†·è™•ç†ã€æˆ–ã€Œè©±èªæ¬Šå¤±è¡¡ã€ã€‚
        
        ã€è¼¸å‡ºæ ¼å¼ (åš´æ ¼éµå®ˆ)ã€‘ï¼š
        ### [DATA_TIMELINE]
        (æ ¼å¼ï¼šYYYY-MM-DD|åª’é«”|æ¨™é¡Œ|Source_ID)
        *è«‹æ³¨æ„ï¼šåªèƒ½åˆ—å‡º Context ä¸­å¯¦éš›å­˜åœ¨çš„ Sourceï¼Œåš´ç¦æé€  Source IDã€‚*
        
        ### [REPORT_TEXT]
        (Markdown å ±å‘Š - ç¹é«”ä¸­æ–‡)
        1. **ğŸ“Š å…¨åŸŸç¾æ³æ‘˜è¦ (Situational Analysis)**
           - è«‹ä»¥ **Markdown è¡¨æ ¼** å‘ˆç¾é—œéµäº‹ä»¶æ™‚é–“è»¸ (æ¬„ä½åŒ…å«ï¼šæ—¥æœŸ | äº‹ä»¶æ‘˜è¦ | é—œéµå½±éŸ¿)ã€‚
        2. **ğŸ” çˆ­è­°é»èˆ‡äº‹å¯¦æŸ¥æ ¸ (Fact-Check & Logic Scan)**
           - *åŒ…å«ï¼šé‚è¼¯è¬¬èª¤åµæ¸¬ã€è­‰æ“šå¼·åº¦è©•ä¼°*
        3. **âš–ï¸ åª’é«”æ¡†æ¶å…‰è­œåˆ†æ (Framing Analysis)**
           - *è«‹æ‡‰ç”¨è²é‡æ¬Šé‡æ ¡æ­£ï¼ŒæŒ‡å‡ºè©±èªæ¬Šæ˜¯å¦å¤±è¡¡*
        4. **ğŸ§  æ·±åº¦è­˜è®€èˆ‡åˆ©ç›Šåˆ†æ (Cui Bono)**
        5. **ğŸ¤” çµæ§‹æ€§åæ€ (Structural Reflection)**
        """
        
    elif mode == "DEEP_SCENARIO":
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼æœªä¾†å­¸ (Futures Studies) çš„æˆ°ç•¥é¡§å•ã€‚
        
        ã€âš ï¸ æ™‚é–“éŒ¨é»ã€‘ï¼šä»Šå¤©æ˜¯ {today_str}ã€‚
        {tone_instruction}
        
        ã€åˆ†æä»»å‹™ã€‘ï¼š
        1. **æ—©æœŸé è­¦æŒ‡æ¨™**ï¼šåˆ—å‡ºç›£æ¸¬è¨Šè™Ÿã€‚
        2. **é©—å±åˆ†æ**ï¼šåæ¨å¤±æ•—è®Šæ•¸ã€‚

        ã€è¼¸å‡ºæ ¼å¼ã€‘ï¼š
        ### [DATA_TIMELINE]
        (ç•™ç©º)
        
        ### [REPORT_TEXT]
        (Markdown å ±å‘Š - ç¹é«”ä¸­æ–‡)
        1. **ğŸ¯ CLA æ·±åº¦è§£æ§‹ (Causal Layered Analysis)**
           - Litany / System / Worldview / Myth
        2. **ğŸ”® æœªä¾†è¶¨å‹¢è·¯å¾‘æ¨¡æ“¬ (Scenario Planning)**
           - **åŸºæº–è·¯å¾‘ (Baseline)** + ğŸš© é è­¦æŒ‡æ¨™
           - **è½‰æŠ˜è·¯å¾‘ (Alternative)** + ğŸš© é è­¦æŒ‡æ¨™
           - **æ¥µç«¯è·¯å¾‘ (Wild Card)** + ğŸš© é è­¦æŒ‡æ¨™
        3. **ğŸ’€ é©—å±åˆ†æ (Pre-mortem Analysis)**
        4. **ğŸ’¡ ç¶œåˆç™¼å±•èˆ‡å› æ‡‰å»ºè­°**
        """
    else:
        system_prompt = f"è«‹é‡å° {query} é€²è¡Œåˆ†æã€‚"

    return call_gemini(system_prompt, context_text, model_name, api_key)

def parse_gemini_data(text):
    data = {"timeline": [], "report_text": ""}
    if not text: return data

    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if "|" in line and len(line.split("|")) >= 3 and (line[0].isdigit() or "20" in line or "Future" in line or "è¿‘æœŸ" in line):
            parts = line.split("|")
            try:
                date = parts[0].strip()
                name = parts[1].strip()
                title = parts[2].strip()
                source_id_str = "0"
                if len(parts) >= 4: 
                    raw_id = parts[3].strip()
                    nums = re.findall(r'\d+', raw_id)
                    if nums: source_id_str = nums[0]
                if "XX" in date or "xx" in date: date = "è¿‘æœŸ"
                
                data["timeline"].append({
                    "date": date,
                    "media": name,
                    "title": title,
                    "source_id": int(source_id_str)
                })
            except: pass

    if "### [REPORT_TEXT]" in text:
        data["report_text"] = text.split("### [REPORT_TEXT]")[1].strip()
    elif "### REPORT_TEXT" in text:
        data["report_text"] = text.split("### REPORT_TEXT")[1].strip()
    else:
        match = re.search(r"(#+\s*.*æ‘˜è¦|1\.\s*.*æ‘˜è¦|#+\s*.*CLA)", text)
        if match:
            data["report_text"] = text[match.start():]
        else:
            data["report_text"] = text

    return data

def create_full_html_report(data_result, scenario_result, sources, blind_mode):
    timeline_html = ""
    if data_result and data_result.get("timeline"):
        rows = ""
        for item in data_result["timeline"]:
            s_id = item.get('source_id', 0)
            
            # [V36.6 Fix] åš´æ ¼éæ¿¾ï¼šç„¡æ•ˆä¾†æºç›´æ¥è·³é
            if s_id == 0 or s_id > len(sources): continue
            
            real_url = "#"
            real_date = "------"
            display_media = "æœªçŸ¥ä¾†æº"
            
            source_data = sources[s_id-1]
            real_url = source_data.get('url', '#')
            
            meta_date = source_data.get('published_date')
            url_date = extract_date_from_url(real_url)
            llm_date = item.get('date')
            
            if meta_date and meta_date != "Missing": real_date = meta_date
            elif url_date: real_date = url_date
            elif llm_date and re.match(r'\d{4}-\d{2}-\d{2}', llm_date) and "XX" not in llm_date: real_date = llm_date
            
            cat = classify_source(real_url)
            label, _ = get_category_meta(cat)
            domain = get_domain_name(real_url)
            
            # [V36.6 Fix] é¡¯ç¤ºçœŸå¯¦åª’é«”åç¨±
            media_name = DOMAIN_NAME_MAP.get(domain, domain)
            emoji = "âšª"
            if "ä¸­åœ‹" in label: emoji = "ğŸ”´"
            elif "æ³›è—" in label: emoji = "ğŸ”µ"
            elif "æ³›ç¶ " in label: emoji = "ğŸŸ¢"
            elif "å®˜æ–¹" in label: emoji = "âšª"
            elif "ç¨ç«‹" in label: emoji = "ğŸ•µï¸"
            elif "åœ‹éš›" in label: emoji = "ğŸŒ"
            elif "è¾²å ´" in label: emoji = "â›”"
            elif "ç¤¾ç¾¤" in label: emoji = "âš ï¸"
            
            display_media = f"{emoji} {media_name}"
            
            title = item.get('title', 'No Title')
            title_html = f'<a href="{real_url}" target="_blank">{title}</a>' if real_url != "#" else title
            rows += f"<tr><td>{real_date}</td><td>{display_media}</td><td>{title_html}</td></tr>"
        
        timeline_html = f"""
        <h3>ğŸ“… é—œéµç™¼å±•æ™‚åº</h3>
        <table class="custom-table" border="1" cellspacing="0" cellpadding="5" style="width:100%; border-collapse:collapse;">
            <thead><tr><th width="120">æ—¥æœŸ</th><th width="140">åª’é«”</th><th>æ–°èæ¨™é¡Œ</th></tr></thead>
            <tbody>{rows}</tbody>
        </table>
        <hr>
        """

    report_html_1 = ""
    if data_result:
        raw_md = data_result.get("report_text", "")
        raw_md = format_citation_style(raw_md)
        html_content = markdown.markdown(raw_md, extensions=['tables'])
        report_html_1 = f'<div class="report-paper"><h3>ğŸ“ å¹³è¡¡å ±å°åˆ†æ</h3>{html_content}</div>'

    report_html_2 = ""
    if scenario_result:
        raw_md_2 = scenario_result.get("report_text", "")
        raw_md_2 = format_citation_style(raw_md_2)
        html_content_2 = markdown.markdown(raw_md_2, extensions=['tables'])
        report_html_2 = f'<div class="report-paper"><h3>ğŸ”® æœªä¾†ç™¼å±•æ¨æ¼”å ±å‘Š</h3>{html_content_2}</div>'

    sources_html = ""
    if sources:
        s_rows = ""
        for i, s in enumerate(sources):
            domain = get_domain_name(s.get('url'))
            media_name = DOMAIN_NAME_MAP.get(domain, domain)
            title = s.get('title', 'No Title')
            url = s.get('url')
            s_rows += f"<li><b>[{i+1}]</b> {media_name} - <a href='{url}' target='_blank'>{title}</a></li>"
        sources_html = f"<hr><h3>ğŸ“š å¼•ç”¨æ–‡ç»åˆ—è¡¨</h3><ul>{s_rows}</ul>"

    full_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <title>å…¨åŸŸè§€é»åˆ†æå ±å‘Š</title>
        {CSS_STYLE}
    </head>
    <body style="padding: 20px; max-width: 900px; margin: 0 auto;">
        <h1>å…¨åŸŸè§€é»åˆ†æå ±å‘Š (V36.6)</h1>
        <p>ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}</p>
        {timeline_html}
        {report_html_1}
        {report_html_2}
        {sources_html}
    </body>
    </html>
    """
    return full_html

def render_html_timeline(timeline_data, sources, blind_mode):
    if not timeline_data: return

    table_rows = ""
    for item in timeline_data:
        s_id = item.get('source_id', 0)
        
        # [V36.6 Fix] åš´æ ¼éæ¿¾ï¼šç„¡æ•ˆä¾†æºç›´æ¥è·³é (UIç‰ˆ)
        if s_id == 0 or s_id > len(sources): continue
        
        real_url = "#"
        real_date = "------"
        
        source_data = sources[s_id-1]
        real_url = source_data.get('url', '#')
        
        meta_date = source_data.get('published_date')
        url_date = extract_date_from_url(real_url)
        llm_date = item.get('date')
        
        if meta_date and meta_date != "Missing": real_date = meta_date
        elif url_date: real_date = url_date
        elif llm_date and re.match(r'\d{4}-\d{2}-\d{2}', llm_date) and "XX" not in llm_date: real_date = llm_date
        
        cat = classify_source(real_url)
        label, _ = get_category_meta(cat)
        domain = get_domain_name(real_url)
        
        # [V36.6 Fix] é¡¯ç¤ºçœŸå¯¦åª’é«”åç¨±
        media_name = DOMAIN_NAME_MAP.get(domain, domain)
        
        emoji = "âšª"
        if "ä¸­åœ‹" in label: emoji = "ğŸ”´"
        elif "æ³›è—" in label: emoji = "ğŸ”µ"
        elif "æ³›ç¶ " in label: emoji = "ğŸŸ¢"
        elif "å®˜æ–¹" in label: emoji = "âšª"
        elif "ç¨ç«‹" in label: emoji = "ğŸ•µï¸"
        elif "åœ‹éš›" in label: emoji = "ğŸŒ"
        elif "è¾²å ´" in label: emoji = "â›”"
        elif "ç¤¾ç¾¤" in label: emoji = "âš ï¸"
        
        display_media = f"{emoji} {media_name}"
        if blind_mode: display_media = "*****"
        
        title = item.get('title', 'No Title')
        title_html = f'<a href="{real_url}" target="_blank">{title}</a>' if real_url != "#" else title
        
        table_rows += f"<tr><td style='white-space:nowrap;'>{real_date}</td><td style='white-space:nowrap;'>{display_media}</td><td>{title_html}</td></tr>"

    full_html = f"""
    <div class="scrollable-table-container">
    <table class="custom-table">
    <thead>
    <tr>
    <th style="width:120px;">æ—¥æœŸ</th>
    <th style="width:180px;">åª’é«”</th>
    <th>æ–°èæ¨™é¡Œ</th>
    </tr>
    </thead>
    <tbody>
    {table_rows}
    </tbody>
    </table>
    </div>
    """
    
    st.markdown("### ğŸ“… é—œéµç™¼å±•æ™‚åº")
    st.markdown(full_html, unsafe_allow_html=True)

def export_full_state():
    data = {
        "result": st.session_state.result,
        "scenario_result": st.session_state.scenario_result,
        "sources": st.session_state.sources
    }
    return json.dumps(data, indent=2, ensure_ascii=False)

def convert_data_to_md(data):
    return f"""
# å…¨åŸŸè§€é»åˆ†æå ±å‘Š (V36.6)
äº§ç”Ÿæ™‚é–“: {datetime.now()}

## 1. å¹³è¡¡å ±å°åˆ†æ
{data.get('report_text')}

## 2. æ™‚é–“è»¸
{pd.DataFrame(data.get('timeline')).to_markdown(index=False)}
    """

# ==========================================
# 5. UI
# ==========================================
with st.sidebar:
    st.title("å…¨åŸŸè§€é»è§£æ V36.6")
    
    analysis_mode = st.radio(
        "é¸æ“‡åˆ†æå¼•æ“ï¼š",
        options=["å…¨åŸŸæ·±åº¦è§£æ (Fusion)", "æœªä¾†ç™¼å±•æ¨æ¼” (Scenario)"],
        captions=["å­¸è¡“æ¡†æ¶ï¼šæ¡†æ¶ + é‚è¼¯åµéŒ¯", "å­¸è¡“æ¡†æ¶ï¼šCLA + é è­¦æŒ‡æ¨™"],
        index=0
    )
    st.markdown("---")
    
    enable_outpost = st.toggle("ğŸ“¡ å‰å“¨ç«™æ¨¡å¼ (ç´å…¥ PTT/Dcard)", value=False)
    blind_mode = st.toggle("ğŸ™ˆ ç›²æ¸¬æ¨¡å¼", value=False)
    
    with st.expander("ğŸ”‘ API è¨­å®š", expanded=True):
        if "GOOGLE_API_KEY" in st.secrets:
            st.success("âœ… Gemini Key Ready")
            google_key = st.secrets["GOOGLE_API_KEY"]
        else:
            google_key = st.text_input("Gemini Key", type="password")

        if "TAVILY_API_KEY" in st.secrets:
            st.success("âœ… Tavily Ready")
            tavily_key = st.secrets["TAVILY_API_KEY"]
        else:
            tavily_key = st.text_input("Tavily Key", type="password")
            
        model_name = st.selectbox(
            "æ¨¡å‹ (Gemini 2.5 Series)", 
            ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite"], 
            index=0
        )
        
        search_days = st.number_input("æœå°‹æ™‚é–“ç¯„åœ (å¤©æ•¸)", min_value=1, max_value=1825, value=30, step=1)
        max_results = st.slider("æœå°‹ç¯‡æ•¸ä¸Šé™", 10, 100, 30)
        
        selected_regions = st.multiselect(
            "æœå°‹è¦–è§’ (Region) - å¯è¤‡é¸",
            ["ğŸ‡¹ğŸ‡¼ å°ç£ (Taiwan)", "ğŸŒ äºæ´² (Asia)", "ğŸŒ æ­æ´² (Europe)", "ğŸŒ ç¾æ´² (Americas)", "ğŸ•µï¸ ç¨ç«‹/è‡ªåª’é«” (Indie)"],
            default=["ğŸ‡¹ğŸ‡¼ å°ç£ (Taiwan)"]
        )

    with st.expander("ğŸ“‚ åŒ¯å…¥èˆŠæƒ…å ± (JSONé‚„åŸ / æ–‡å­—è²¼ä¸Š)", expanded=False):
        uploaded_file = st.file_uploader("ä¸Šå‚³æª”æ¡ˆ", type=["json", "md", "txt"])
        default_text = ""
        is_json_upload = False
        if uploaded_file:
            try:
                if uploaded_file.name.endswith(".json"):
                    is_json_upload = True
                    st.success(f"âœ… å®Œæ•´å­˜æª”: {uploaded_file.name}")
                else:
                    default_text = uploaded_file.getvalue().decode("utf-8")
                    st.success(f"âœ… æ–‡å­—æª”: {uploaded_file.name}")
            except: pass

        past_report_input = st.text_area("æˆ–è²¼ä¸Šå…§å®¹ï¼š", value=default_text, height=150)
        
        if uploaded_file and st.button("ğŸ”„ ç¢ºèªè¼‰å…¥/é‚„åŸ"):
            if is_json_upload:
                try:
                    state_data = json.load(uploaded_file)
                    st.session_state.result = state_data.get("result")
                    st.session_state.scenario_result = state_data.get("scenario_result")
                    st.session_state.sources = state_data.get("sources")
                    st.rerun()
                except: st.error("JSON è§£æå¤±æ•—")
            else:
                st.toast("âœ… æ–‡å­—å·²åŒ¯å…¥")

    st.markdown("### ğŸ§  æƒ…å ±åˆ†ææ–¹æ³•è«–è©³è§£")
    
    with st.expander("1. è³‡è¨Šæª¢ç´¢ï¼šæ··å’Œæ¬Šé‡èˆ‡ä¸‰è»Œæœå°‹ (Hybrid Weighted Search)"):
        st.markdown("""
        **æ ¸å¿ƒæ©Ÿåˆ¶ï¼šæ··å’Œæ¬Šé‡æœå°‹**
        - **åˆ†çœ¾ä¿åº• (Safety Net)**ï¼šå¼·åˆ¶é–‹å•Ÿå°ˆç”¨é€šé“ï¼Œç¢ºä¿è—ç‡Ÿã€ç¶ ç‡Ÿã€å®˜æ–¹è‡³å°‘å„æŠ“å– 5 ç¯‡ä»£è¡¨æ€§æ–‡ç« ï¼Œä¿éšœå¼±å‹¢è§€é»å…¥å ´ã€‚
        - **ç†±åº¦è£œå®Œ (Volume Fill)**ï¼šå‰©é¤˜åé¡é–‹æ”¾çµ¦å…¨ç¶²ç†±åº¦æ’åºï¼Œåæ˜ çœŸå¯¦è¼¿è«–è²é‡ã€‚
        
        **ä¸‰è»Œæœå°‹æ¶æ§‹ (Tri-Track via Dynamic Keywords)**
        å°‡ã€Œé€šç”¨æœå°‹ (General)ã€ä»»å‹™æ‹†è§£ç‚ºä¸‰çµ„ä¸åŒç›®çš„çš„æŒ‡ä»¤ï¼Œç¢ºä¿æŠ“å–å…§å®¹çš„ç¶­åº¦å®Œæ•´ï¼š
        1. **äº‹å¯¦èˆ‡æ™‚åº (Facts & Timeline)**
           - æŒ‡ä»¤ï¼š`{query} æ–°è äº‹ä»¶ æ™‚é–“è»¸`
           - ä»»å‹™ï¼šåªé—œå¿ƒã€Œç™¼ç”Ÿäº†ä»€éº¼äº‹ï¼Ÿã€ã€Œä»€éº¼æ™‚å€™ç™¼ç”Ÿçš„ï¼Ÿã€ã€‚å®ƒè² è²¬æŠ“å–ç¡¬è³‡è¨Šï¼Œæ§‹å»ºæ™‚é–“è»¸è¡¨æ ¼ã€‚
           - ç›®æ¨™ï¼šç¢ºä¿å ±å‘Šçš„éª¨æ¶ï¼ˆäººã€äº‹ã€æ™‚ã€åœ°ã€ç‰©ï¼‰æ˜¯æº–ç¢ºçš„ã€‚
        2. **è§€é»èˆ‡çˆ­è­° (Opinions & Controversy)**
           - æŒ‡ä»¤ï¼š`{query} è©•è«– è§€é» çˆ­è­° åˆ†æ`
           - ä»»å‹™ï¼šå°ˆé–€å°‹æ‰¾ã€Œåµæ¶çš„é»ã€ã€‚å®ƒæœƒåˆ»æ„å»æŠ“ç¤¾è«–ã€æŠ•æ›¸ã€æ”¿è«–ç¯€ç›®çš„æ‘˜è¦ã€‚
           - ç›®æ¨™ï¼šæ•æ‰ä¸åŒé™£ç‡Ÿï¼ˆæ­£æ–¹/åæ–¹ï¼‰çš„è«–è¿°é‚è¼¯ï¼Œé€™æ˜¯ Entman æ¡†æ¶åˆ†æçš„åŸæ–™ã€‚
        3. **æ·±åº¦èˆ‡çµæ§‹ (Deep Dive)**
           - æŒ‡ä»¤ï¼š`{query} æ‡¶äººåŒ… é‡é» å½±éŸ¿`
           - ä»»å‹™ï¼šå°‹æ‰¾å·²ç¶“è¢«æ•´ç†éçš„çµæ§‹åŒ–è³‡è¨Šï¼ˆå¦‚ï¼šäº”å¤§çˆ­è­°é»ã€æ³•æ¢æ¯”è¼ƒè¡¨ï¼‰ã€‚
           - ç›®æ¨™ï¼šå¿«é€Ÿç²å–è­°é¡Œçš„å…¨è²Œèˆ‡èƒŒæ™¯çŸ¥è­˜ã€‚
        """)
        
    with st.expander("2. æ¡†æ¶åˆ†æï¼šEntman ç†è«–èˆ‡ç«‹å ´åˆ¤å®š (Framing)"):
        st.markdown("""
        **Entman æ¡†æ¶ç†è«– (Framing Theory)**
        æˆ‘å€‘åˆ†ææ–‡æœ¬å¦‚ä½•é€éã€Œé¸æ“‡ (Selection)ã€èˆ‡ã€Œå‡¸é¡¯ (Salience)ã€ä¾†å»ºæ§‹ç¾å¯¦ã€‚
        - **å•é¡Œå®šç¾©**ï¼šä¸åŒé™£ç‡Ÿå¦‚ä½•å®šç¾©å•é¡Œçš„æ ¸å¿ƒï¼Ÿ
        - **æ­¸å› åˆ†æ**ï¼šå°‡è²¬ä»»æ­¸å’æ–¼èª°ï¼Ÿ
        - **é“å¾·è©•åƒ¹**ï¼šä½¿ç”¨ä»€éº¼æ¨£çš„å½¢å®¹è©ä¾†é€²è¡Œé“å¾·å¯©åˆ¤ï¼Ÿ
        
        **æ©Ÿæ§‹å±¤æ¬¡é©—è­‰**
        çµåˆåª’é«”æ‰€æœ‰æ¬Šçµæ§‹ (Ownership) èˆ‡éå¾€æ”¿æ²»å‚¾å‘è³‡æ–™åº« (DB_MAP)ï¼Œå°æ–‡ç« ç«‹å ´é€²è¡Œé›™é‡é©—è­‰ã€‚
        """)
        
    with st.expander("3. å¯ä¿¡åº¦é©—è­‰ï¼šæ°´å¹³é–±è®€èˆ‡é‚è¼¯åµéŒ¯ (Verification)"):
        st.markdown("""
        **æ°´å¹³é–±è®€æ³• (Lateral Reading)**
        æ¡ç”¨å²ä¸¹ä½›æ­·å²æ•™è‚²ç¾¤ (SHEG) æå€¡ä¹‹æ–¹æ³•ï¼Œä¸åªæ·±è®€å–®ä¸€ä¾†æºï¼Œè€Œæ˜¯æ©«å‘æ¯”å°å¤šå€‹ä¾†æºä»¥ç¢ºèªäº‹å¯¦ã€‚
        
        **é‚è¼¯åµéŒ¯ (Logic Scan)**
        AI æœƒè‡ªå‹•æƒææ–‡æœ¬ä¸­çš„é‚è¼¯è¬¬èª¤ï¼š
        - **æ»‘å¡è¬¬èª¤**ï¼šèª‡å¤§å¾®å°è¡Œå‹•çš„ç½é›£æ€§å¾Œæœã€‚
        - **ç¨»è‰äººè«–è­‰**ï¼šæ‰­æ›²å°æ‰‹è§€é»ä»¥ä¾¿æ”»æ“Šã€‚
        
        **Cofacts å”ä½œæŸ¥æ ¸**
        å³æ™‚ä¸²æ¥ g0v Cofacts è¬ è¨€è³‡æ–™åº«ï¼Œæ¨™è¨»å·²è¢«ç¤¾ç¾¤æŸ¥æ ¸ç‚ºéŒ¯èª¤çš„è³‡è¨Šã€‚
        """)
        
    with st.expander("4. æˆ°ç•¥æ¨æ¼”ï¼šCLA å±¤æ¬¡åˆ†æèˆ‡é è­¦ (Futures)"):
        st.markdown("""
        **CLA å±¤æ¬¡åˆ†ææ³• (Causal Layered Analysis)**
        æ·±å…¥æŒ–æ˜è­°é¡Œçš„å››å€‹å±¤æ¬¡ï¼š
        1. **è¡¨è±¡ (Litany)**ï¼šå…¬çœ¾çœ‹åˆ°çš„äº‹ä»¶èˆ‡æ•¸æ“šã€‚
        2. **ç³»çµ± (System)**ï¼šé€ æˆäº‹ä»¶çš„ç¤¾æœƒçµæ§‹èˆ‡æ”¿ç­–æˆå› ã€‚
        3. **ä¸–ç•Œè§€ (Worldview)**ï¼šåˆ©ç›Šç›¸é—œè€…çš„æ·±å±¤åƒ¹å€¼è§€èˆ‡æ„è­˜å½¢æ…‹ã€‚
        4. **ç¥è©±/éš±å–» (Myth)**ï¼šæ½›æ„è­˜ä¸­çš„é›†é«”ç„¦æ…®æˆ–æ•…äº‹åŸå‹ã€‚
        
        **æ—©æœŸé è­¦æŒ‡æ¨™ (Signposts)**
        ç‚ºæ¯å€‹æœªä¾†æƒ…å¢ƒè¨­å®šå…·é«”çš„ç›£æ¸¬è¨Šè™Ÿã€‚
        
        **é©—å±åˆ†æ (Pre-mortem)**
        å‡è¨­é æ¸¬å¤±æ•—ï¼Œåæ¨å¯èƒ½çš„éš±è”½è®Šæ•¸ã€‚
        """)
        
    st.markdown("### ğŸ“¥ å ±å‘ŠåŒ¯å‡º")
    if st.session_state.get('result') or st.session_state.get('scenario_result'):
        html_report = create_full_html_report(st.session_state.result, st.session_state.scenario_result, st.session_state.sources, blind_mode)
        st.download_button("ğŸ“¥ åˆ—å°ç”¨æª”æ¡ˆ (HTML)", html_report, "Printable_Report.html", "text/html")
        full_state_json = export_full_state()
        st.download_button("ğŸ“¥ å®Œæ•´ç‹€æ…‹ (JSON)", full_state_json, "Full_State.json", "application/json")
        
        export_data = st.session_state.get('result').copy()
        if st.session_state.get('scenario_result'):
            export_data['report_text'] += "\n\n# æœªä¾†ç™¼å±•æ¨æ¼”å ±å‘Š\n" + st.session_state.get('scenario_result')['report_text']
        st.download_button("ğŸ“¥ ç´”æ–‡å­— (Markdown)", convert_data_to_md(export_data), "report.md", "text/markdown")

st.title(f"{analysis_mode.split(' ')[0]}")
query = st.text_input("è¼¸å…¥è­°é¡Œé—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ç¾åœ‹è¨­å» çˆ­è­°")
search_btn = st.button("ğŸš€ å•Ÿå‹•å…¨åŸŸæƒæ", type="primary")

if 'result' not in st.session_state: st.session_state.result = None
if 'scenario_result' not in st.session_state: st.session_state.scenario_result = None
if 'sources' not in st.session_state: st.session_state.sources = None

if search_btn and query and google_key and tavily_key:
    st.session_state.result = None
    st.session_state.scenario_result = None
    
    with st.status("ğŸš€ å•Ÿå‹• V36.6 å¹³è¡¡å ±å°åˆ†æå¼•æ“...", expanded=True) as status:
        
        st.write("ğŸ§  1. ç”Ÿæˆå‹•æ…‹æœå°‹ç­–ç•¥...")
        dynamic_keywords = generate_dynamic_keywords(query, google_key)
        st.write(f"   â†³ é–å®šæˆ°ç•¥é—œéµå­—: {', '.join(dynamic_keywords)}")
        
        regions_label = ", ".join([r.split(" ")[1] for r in selected_regions])
        st.write(f"ğŸ“¡ 2. åŸ·è¡Œæ··å’Œæ¬Šé‡æœå°‹ (è¦–è§’: {regions_label})...")
        st.write("   â†³ å•Ÿå‹•æ©Ÿåˆ¶ï¼šåˆ†çœ¾ä¿åº• (è—/ç¶ /å®˜æ–¹å„5ç¯‡) + ç†±åº¦è£œå®Œ (å‹•æ…‹ä¸‰è»Œ)")
        
        context_text, sources, actual_query, is_strict_tw = get_search_context(
            query, tavily_key, search_days, selected_regions, max_results, enable_outpost, dynamic_keywords
        )
        
        st.write(f"   â†³ æœå°‹å®Œæˆï¼šå…±ç²å– {len(sources)} ç¯‡è³‡æ–™ (å·²å»é‡)ã€‚")
        if is_strict_tw:
            st.write(f"ğŸ›¡ï¸ ç¶²åŸŸåœç±¬å·²å•Ÿå‹•ã€‚")
        
        st.session_state.sources = sources
        
        st.write("ğŸ›¡ï¸ 3. æŸ¥è©¢ Cofacts è¬ è¨€è³‡æ–™åº«...")
        cofacts_txt = search_cofacts(query)
        if cofacts_txt: context_text += f"\n{cofacts_txt}\n"
        
        st.write("ğŸ§  4. AI é€²è¡Œæ·±åº¦æˆ°ç•¥åˆ†æ (ACH ç«¶çˆ­å‡è¨­ + é‚è¼¯åµéŒ¯)...")
        
        mode_code = "DEEP_SCENARIO" if "æœªä¾†" in analysis_mode else "FUSION"
        analysis_context = past_report_input if (mode_code == "DEEP_SCENARIO" and past_report_input) else context_text

        raw_report = run_strategic_analysis(query, analysis_context, model_name, google_key, mode=mode_code)
        st.session_state.result = parse_gemini_data(raw_report)
            
        status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
        
    st.rerun()

if st.session_state.result:
    data = st.session_state.result
    render_html_timeline(data.get("timeline"), st.session_state.sources, blind_mode)

    st.markdown("---")
    st.markdown("### ğŸ“ å¹³è¡¡å ±å°åˆ†æ")
    formatted_text = format_citation_style(data.get("report_text", ""))
    html_content = markdown.markdown(formatted_text, extensions=['tables'])
    st.markdown(f'<div class="report-paper">{html_content}</div>', unsafe_allow_html=True)
    
    if "æœªä¾†" not in analysis_mode and not st.session_state.scenario_result:
        st.markdown("---")
        if st.button("ğŸš€ å°‡æ­¤çµæœé¤µçµ¦æœªä¾†ç™¼å±•æ¨æ¼” (è³‡è¨Šæ»¾å‹•)", type="secondary"):
            with st.spinner("ğŸ”® æ­£åœ¨è®€å–å‰æ¬¡æƒ…å ±ï¼Œå•Ÿå‹• CLA å±¤æ¬¡åˆ†æèˆ‡æœªä¾†æ¨æ¼”..."):
                current_report = data.get("report_text", "")
                raw_text = run_strategic_analysis(query, current_report, model_name, google_key, mode="DEEP_SCENARIO")
                st.session_state.scenario_result = parse_gemini_data(raw_text) 
                st.rerun()

if st.session_state.scenario_result:
    st.markdown("---")
    st.markdown("### ğŸ”® æœªä¾†ç™¼å±•æ¨æ¼”å ±å‘Š")
    scenario_data = st.session_state.scenario_result
    formatted_scenario = format_citation_style(scenario_data.get("report_text", ""))
    html_scenario = markdown.markdown(formatted_scenario, extensions=['tables'])
    st.markdown(f'<div class="report-paper">{html_scenario}</div>', unsafe_allow_html=True)

if st.session_state.sources:
    st.markdown("---")
    st.markdown("### ğŸ“š å¼•ç”¨æ–‡ç»åˆ—è¡¨")
    md_table = "| ç·¨è™Ÿ | åª’é«”/ç¶²åŸŸ | æ¨™é¡Œæ‘˜è¦ | é€£çµ |\n|:---:|:---|:---|:---|\n"
    for i, s in enumerate(st.session_state.sources):
        domain = get_domain_name(s.get('url'))
        if blind_mode: domain = "*****"
        
        title = s.get('title', 'No Title')
        if len(title) > 60: title = title[:60] + "..."
        url = s.get('url')
        md_table += f"| **{i+1}** | `{domain}` | {title} | [é»æ“Š]({url}) |\n"
    st.markdown(md_table)
